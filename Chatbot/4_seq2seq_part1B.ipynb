{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用raw_rnn实现自定义的decoding功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helpers\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell, LSTMStateTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder部分，使用双向encoder \n",
    "来自上一部分代码演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "vocab_size = 10\n",
    "input_embedding_size = 20\n",
    "\n",
    "encoder_hidden_units = 20\n",
    "decoder_hidden_units = 2 * encoder_hidden_units # bidirectional RNN, 见后面的解释\n",
    "\n",
    "\n",
    "with tf.name_scope('minibatch'):\n",
    "    encoder_inputs = tf.placeholder(\n",
    "        shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "    encoder_inputs_length = tf.placeholder(\n",
    "        shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "\n",
    "    decoder_targets = tf.placeholder(\n",
    "        shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "\n",
    "with tf.name_scope('embedding'):\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0),\n",
    "        dtype=tf.float32)\n",
    "\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(\n",
    "    embeddings, encoder_inputs)\n",
    "\n",
    "encoder_cell = BasicLSTMCell(encoder_hidden_units)\n",
    "\n",
    "# 双向dynamic_rnn\n",
    "((encoder_fw_outputs, encoder_bw_outputs),\n",
    " (encoder_fw_final_state, encoder_bw_final_state)) = (\n",
    "    tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw=encoder_cell,\n",
    "        cell_bw=encoder_cell,\n",
    "        inputs=encoder_inputs_embedded,\n",
    "        sequence_length=encoder_inputs_length,\n",
    "        dtype=tf.float32, time_major=True)\n",
    ")\n",
    "\n",
    "# 将正向和反向的encoder的final state拼接到一起，需要对state.c和state.h分别操作\n",
    "encoder_final_state_c = tf.concat(\n",
    "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "encoder_final_state_h = tf.concat(\n",
    "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用raw_rnn实现自定义的decoding功能:\n",
    "\n",
    "## 不提供真实的句子作为`decoder_input`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder via `tf.nn.raw_rnn`\n",
    "\n",
    "**`dynamic_rnn`**需要提供decoder_input\n",
    "1. 提供真实的sequence作为decoder_input，与testing环境相差太大\n",
    "2. 提供`[<pad>, <pad>, ... <pad>]`作为decoder_input，浪费input信息\n",
    "\n",
    "\n",
    "\n",
    "![seq2seq-feed-previous](figure/nct-seq2seq.png)\n",
    "* **这个图片的操作在训练和测试阶段都可以使用**\n",
    "* 图片来自 http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_cell = BasicLSTMCell(decoder_hidden_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`encoder_max_time`**和**`batch_size`**这两个Tensor取决于placeholder输入的数据的维度。注意代码里面的placeholder的形状是**`None`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义`decoder_lengths`，作为decoder输出sequence的预期长度的上限\n",
    "\n",
    "## 实际的decoding结果的长度取决于`<eos>`出现的位置\n",
    "\n",
    "### 1. 如果`<eos>`出现的位置早于达到长度上限，则之后的decoding cell 输出的内容忽略不计\n",
    "### 2. 如果达到decoding 长度上限以后`<eos>`没有出现，则强行停止decoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "loop_fn_initial()\n",
    "\n",
    "while current_length[i] <= max_decoding_length[i] for some i:\n",
    "\n",
    "    (previous_output, previous_state) = loop_fn_transition ( previous_output, previous_state)\n",
    "    \n",
    "     current_length[i] += 1 for every i\n",
    "\n",
    "```\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_lengths = encoder_inputs_length + 3\n",
    "# 这里定义每一个样本的decoding的输出序列的上限比这个样本encoder输入序列的长度大一些\n",
    "# 例如，\n",
    "# encoder_inputs_length = [2, 3, 5, 4]\n",
    "# decoder_length = [5, 6, 8, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert EOS == 1 and PAD == 0\n",
    "\n",
    "eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
    "pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
    "\n",
    "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
    "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 用于从当前RNN output预测最可能的单词\n",
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1), dtype=tf.float32)\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n",
    "    initial_input = eos_step_embedded\n",
    "    initial_cell_state = encoder_final_state\n",
    "    initial_cell_output = None\n",
    "    initial_loop_state = None  # we don't need to pass any additional information\n",
    "    \n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](figure/loop_fn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "\n",
    "    def get_next_input():\n",
    "        \"\"\"产生下一个时间的输入单词\n",
    "        \n",
    "        根据当前时间的rnn output,对minibatch中的每一个样本预测这个时间最可能的单词，\n",
    "        将其作为下一个时间的decoding任务的输入单词\n",
    "        \"\"\"\n",
    "        output_logits = tf.add(tf.matmul(previous_output, W), b) # [batch_size, vocabSize]\n",
    "        prediction = tf.argmax(output_logits, axis=1) # [batch_size]\n",
    "        next_input = tf.nn.embedding_lookup(embeddings, prediction) # [batch_size, embed_dim]\n",
    "        return next_input\n",
    "    \n",
    "    elements_finished = (time >= decoder_lengths)\n",
    "    # boolen tensor of [batch_size], 决定是否每一个样本都完成了decoding任务\n",
    "    # 对batch_size中的每一个，如果time大于预期的decoding_length，则该样本的decoding任务已经完成\n",
    "\n",
    "    finished = tf.reduce_all(elements_finished)\n",
    "    # 当且尽当所有的 elements_finished 元素都是 True,即，所有样本都已经完成decoding时，finished = True\n",
    "    \n",
    "    input = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "    # return pad_step_embedded if finished=True,\n",
    "    #   lambda: pad_step_embedded, batch_size个[pad] 的 embedding\n",
    "    #   lambda的用初始将tensor pad_step_embedded转化成一个函数，满足tf.cond()的 Arg 要求\n",
    "    # return get_next_input if finished=False\n",
    "    #   get_next_input 用来产生下一个时间的input单词的embedding\n",
    "    \n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "\n",
    "    return (elements_finished, \n",
    "            input,\n",
    "            state,\n",
    "            output,\n",
    "            loop_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "    if previous_state is None:    # time == 0\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用raw_rnn()实现自定义rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
    "decoder_outputs = decoder_outputs_ta.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x1272da0f0>\n",
      "Tensor(\"TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, ?, 40), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_outputs_ta)\n",
    "print(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN decoding的输出 `decoder_outputs` ，维度[max_time, batch_size, hidden_units]的Tensor，使用下面的参数W, b映射为维度为 [max_time, batch_size, vocab_size]的`decoder_logits` 其中`vocab_size`是固定数值的超参数，`max_time`, `batch_size`有feed给tf的数据决定."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(\n",
    "    tf.shape(decoder_outputs))\n",
    "decoder_outputs_flat = tf.reshape(\n",
    "    decoder_outputs, (-1, decoder_dim))\n",
    "decoder_logits_flat = tf.add(\n",
    "    tf.matmul(decoder_outputs_flat, W), b)\n",
    "decoder_logits = tf.reshape(\n",
    "    decoder_logits_flat, \n",
    "    (decoder_max_steps, decoder_batch_size, vocab_size))\n",
    "\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head of the batch:\n",
      "[7, 8, 9, 6, 8]\n",
      "[6, 3, 5, 9, 8, 8, 9]\n",
      "[6, 6, 9, 5, 2, 6, 6]\n",
      "[3, 2, 6]\n",
      "[9, 6, 6, 5, 7, 5, 7, 6]\n",
      "[4, 6, 6, 7]\n",
      "[6, 4, 7]\n",
      "[2, 8, 3, 9, 5, 8, 7]\n",
      "[2, 4, 5]\n",
      "[6, 7, 6, 4, 4, 6, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "batches = helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "print('head of the batch:')\n",
    "for seq in next(batches)[:10]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    encoder_inputs_, encoder_input_lengths_ = helpers.batch(batch)\n",
    "    decoder_targets_, _ = helpers.batch(\n",
    "        [(sequence) + [EOS] + [PAD] * 2 for sequence in batch]\n",
    "    )\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        encoder_inputs_length: encoder_input_lengths_,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.3143234252929688\n",
      "  sample 1:\n",
      "    input     > [5 5 6 7 9 8 9 0]\n",
      "    predicted > [0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [2 6 7 0 0 0 0 0]\n",
      "    predicted > [1 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [2 6 7 6 7 0 0 0]\n",
      "    predicted > [1 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 0.528678834438324\n",
      "  sample 1:\n",
      "    input     > [6 4 8 3 3 4 8 0]\n",
      "    predicted > [6 4 3 3 8 4 3 1 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 4 7 7 7 9 8 0]\n",
      "    predicted > [5 4 7 7 7 7 9 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [8 4 6 3 8 3 6 8]\n",
      "    predicted > [8 4 3 6 8 8 8 8 1 0 0]\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 0.26201632618904114\n",
      "  sample 1:\n",
      "    input     > [3 3 6 9 9 0 0 0]\n",
      "    predicted > [3 3 6 9 9 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [8 4 6 9 4 3 6 9]\n",
      "    predicted > [8 4 6 4 6 6 9 3 1 0 0]\n",
      "  sample 3:\n",
      "    input     > [3 8 6 2 0 0 0 0]\n",
      "    predicted > [3 8 6 2 1 0 0 0 0 0 0]\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 0.13954396545886993\n",
      "  sample 1:\n",
      "    input     > [4 5 4 0 0 0 0 0]\n",
      "    predicted > [4 5 4 1 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 8 4 6 0 0 0 0]\n",
      "    predicted > [5 8 4 6 1 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 6 8 6 0 0 0 0]\n",
      "    predicted > [6 6 8 6 1 0 0 0 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_batches = 3001\n",
    "batches_in_epoch = 1000\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.1397 after 300100 examples (batch_size=100)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8VNXdx/HPLzskBATCIothFUFFFFFAUKwLglbrY32w\ntWhbl1pqLW21qFWpS+VprbVYq8V9X+pKBTcUBWQRUPY1YJA9hCUhkD3n+WOGkJCEhDDJnbnzfb9e\neXHnzp2Z3/Gab+6ce+495pxDRET8JcbrAkREJPQU7iIiPqRwFxHxIYW7iIgPKdxFRHxI4S4i4kMK\ndxERH1K4i4j4kMJdRMSH4rz64NatW7v09HSvPl5EJCItXLgw2zmXVtt2noV7eno6CxYs8OrjRUQi\nkpltqMt26pYREfEhhbuIiA8p3EVEfEjhLiLiQwp3EREfUriLiPiQwl1ExIciLtxXbcvloY9Ws2tf\nkdeliIiErYgL98zsffxzegbbcgq8LkVEJGxFXLinJsUDkJNf7HElIiLhK+LCvUlCLAD5xSUeVyIi\nEr4iLtyT4gPhXlBc5nElIiLhK4LDvdTjSkREwlcEhnug5MISHbmLiNQk8sI9TkfuIiK1ibhwTwwe\nuavPXUSkZhEX7jpyFxGpXcSFe0yMkRAbQ0GJwl1EpCYRF+4QOKlaUKRwFxGpSUSGe4umCezRFaoi\nIjWKyHBvlZLAzjzdOExEpCaRGe7JiWTnFXpdhohI2IrIcG+dksBO3fJXRKRGERnurVIS2LWviLIy\n53UpIiJhKTLDPTmR0jKn2/6KiNQgMsM9JQGARRv3eFyJiEh4ishwb52SCMBPn5vPe4s2e1yNiEj4\nichw79m2WfnyLa8t8rASEZHwFJHhntYs0esSRETCWkSGO8Avzu7mdQkiImErYsN93EW9ypd1h0gR\nkcoiNtwryi3QkEgRkYoiOtxHnd4JgL0FJR5XIiISXiI63C/o0xaAF2ZneluIiEiYiehwT06IA+D5\nORs8rkREJLzUGu5m1snMppvZCjNbbma3VLONmdlEM8swsyVmdmrDlFtZQlxE/20SEWkwcXXYpgT4\nnXPuazNrBiw0s0+ccysqbHMR0CP4cwbwePDfBpUYnE9VREQqq/XQ1zm31Tn3dXB5L7AS6HDIZpcC\nL7iAuUALM2sf8moP0atds9o3EhGJQkfUr2Fm6UA/YN4hT3UANlZ4vImqfwBCLibGypdLdftfEZFy\ndQ53M0sB3gJ+45zLrc+HmdkNZrbAzBbs2LGjPm9Rxc3ndgdgz35N3iEickCdwt3M4gkE+8vOuber\n2WQz0KnC447BdZU45yY55/o75/qnpaXVp94qjg92zWRrTlURkXJ1GS1jwNPASufcwzVsNhkYHRw1\ncyaQ45zbGsI6a5QWvP3vhY/MoLBEtyEQEYG6jZYZDPwEWGpmB+6vewfQGcA59wQwFRgBZAD7gZ+G\nvtTqdWuTUr6cX1SqETQiItQh3J1zswCrZRsHjAlVUUfiwMQdABt35dOiaYIXZYiIhBVfXQU0d/1O\nr0sQEQkLvgj3j8cOBWBHXqHHlYiIhAdfhHuLJvEATJqx3uNKRETCgy/CvWVyoJ/dDntmQEQkevgi\n3ONiY2jRNB7nIHBuV0Qkuvki3AH27A/MxrQ9V/3uIiK+CfcD8go1K5OIiO/CPSdf86mKiPgm3P9y\nxckA5OTrHjMiIr4J90HdWgHwgqbcExHxT7h3aNEEgM9Xh+ZWwiIikcw34W5m3Di0K3ExpuGQIhL1\nfBPuELj9QEmZ44s1OnoXkejmq3AvLCkD4JMV2z2uRETEW74K9xuHdgXgPws2eVyJiIi3fBXufY5t\nDkCX1skeVyIi4i1fhXtsTODOYau37/W4EhERb/kq3EVEJMB34f6zwV1ISazL1LAiIv7lu3BvlhRH\nXmGJ7jEjIlHNd+E+bWVgGOSED1Z5XImIiHd8F+4DurQEICned00TEakz3yXgjUO7AdCjTTOPKxER\n8Y7vwj05MRaAbTn5HlciIuId34V704TASJk563d6XImIiHd8F+6xMUavds00l6qIRDVfDgjPzisk\nO6+I0jJXftWqiEg08d2RO0B2XmCqvQ+WbfW4EhERb/gy3B++si8AxaVlHlciIuINX4b7zuCR+2/f\nWOxxJSIi3vBluJ/fuy0Ao07v7HElIiLe8GW4t2ueBMCH6nMXkSjly3BPjAs0a/d+3TxMRKKTL8Pd\nzDi1cwsSYmMoK3NelyMi0uh8Ge4Al5/akaLSMjbv0W0IRCT6+Dbcd+0LjJiZ+OlajysREWl8tYa7\nmT1jZllmtqyG588xsxwzWxT8uTv0ZR65H/bvCEDb1CSPKxERaXx1uf3Ac8A/gRcOs81M59zFIako\nRNqlJpEQG0NxmS5kEpHoU+uRu3NuBrCrEWoJKTMjPtbYtFt97iISfULV5z7IzJaY2Qdm1qemjczs\nBjNbYGYLduzYEaKPrtm+olKmLNFYdxGJPqEI96+Bzs65k4FHgXdr2tA5N8k519851z8tLS0EHy0i\nItU56nB3zuU65/KCy1OBeDNrfdSVhcBN5wSm3NuqWZlEJMocdbibWTszs+DygOB7hsU0SC2bJgDw\n46fmeVyJiEjjqnW0jJm9CpwDtDazTcA9QDyAc+4J4ArgJjMrAfKBUc65sLgstHnTeADW79jncSUi\nIo2r1nB3zl1Vy/P/JDBUMuxcesqx3PbmEq/LEBFpdL69QhUgMS7W6xJERDzh63CvKCMrz+sSREQa\nTdSE+0tzN3hdgohIo/F9uHdo0QSA1+dv9LgSEZHG4/twv35IFwDyi0s9rkREpPH4PtyvGZRevhwm\nIzRFRBqc78M9eH0VAI9Nz/CwEhGRxuP7cAc4rlVTAB76eI3HlYiINI6oCPcnrj6tfPnADE0iIn4W\nFeF+4MgdYPCEzzysRESkcURFuDdNOHiXBY2aEZFoEBXhDpCccPBWBDn5xR5WIiLS8KIm3Kf8esjB\nZc3OJCI+FzXhnt46uXx5e24B+UXqnhER/4qacAc4oX0qAP/4dC397//E42pERBpOVIX7e2MGly/v\n05G7iPhYVIV7Qlzl5uYW6MSqiPhTVIU7wK+/16N8+eTxH7Np934PqxERaRhRF+6/Pb8n/Tq3KH/8\n1be7PKxGRKRhRF24A7z48zPKl3/7xmL2qntGRHwmKsM9JbHyvOBXPzXPo0pERBpGVIb7oRZvyvG6\nBBGRkIracP/2wRGVHs9el+1RJSIioRe14W5mTPrJwVsB/+jJeWTlFnhYkYhI6ERtuANc0KddpccD\n/vwps9bqCF5EIl9UhzvAnNvPrfT46qd1clVEIl/Uh3v75k247JRjK617cOpKj6oREQmNqA93gDtG\nnlDp8b9nrOfb7H0eVSMicvQU7kCbZkn8YXivSusu/9eXHlUjInL0FO5BP+jXgfhYK3+8e38x/5i2\n1sOKRETqT+Ee1K55EmsfGMETV59avu7v09bw2PQMTcsnIhFH4X6I4Se25wf9OpQ//utHq+n7p489\nrEhE5Mgp3KvxlytOrrLu89VZHlQiIlI/CvdqxMfGVDp6B7j22flk5xV6VJGIyJFRuNfg4Sv7VlnX\n//5pFBRrej4RCX+1hruZPWNmWWa2rIbnzcwmmlmGmS0xs1Or2y7SmBmL77mAm8/tXmn9w5+s8agi\nEZG6q8uR+3PA8MM8fxHQI/hzA/D40ZcVHpo3ied3FxzPnSMOXuQ0acZ6Rk6cycZd+3HOeVidiEjN\nag1359wM4HBz0V0KvOAC5gItzKx9qAoMB9cN6cK9l/Ypf7x8Sy5D/jKdZ7/M9K4oEZHDCEWfewdg\nY4XHm4LrfMPMGD0wvcr6e99fQX6R+uBFJPw06glVM7vBzBaY2YIdO3Y05keHxPs3n8X1Q7pUWnfC\n3R/yxvyNNbxCRMQboQj3zUCnCo87BtdV4Zyb5Jzr75zrn5aWFoKPblwndmjOnSN789nvzq60/ra3\nllBapv53EQkfoQj3ycDo4KiZM4Ec59zWELxv2OqallJlXbc7prJ8i+ZiFZHwUJehkK8Cc4DjzWyT\nmf3czH5hZr8IbjIVWA9kAE8Cv2ywasPIJ2OH8rPBlbtoRk6cxfOzM70pSESkAvNqOF///v3dggUL\nPPnsUHp+dib3TF5ead2HvxlCr3apHlUkIn5mZgudc/1r205XqB6lawalM/6S3pXWDX9kJlOWbNU4\neBHxjMI9BK4ZlM7wQybbHvPK14yYOMujikQk2incQ8DMeOInp/Hx2KGV1q/cmstv31ikI3gRaXQK\n9xDq2bZZlXVvf72ZYQ99ztz1Oz2oSESilcI9xN6/+awq6zJ37mfUpLnMzzzcXRxEREJH4R5iJ3Zo\nzjd3nc+LPx9Q5bkfPjGHKx6f7UFVIhJtFO4N4JjkBIb0SGPiVf2qPLdgw27Sx01h+ZYcvszIZsAD\n08jaW+BBlSLiZwr3BvT9vsdyeb/q76E2cuIsfvzUPLL2FvLZSk3hJyKhpXBvYA//7yl0atnksNuY\nwfZcHb2LSOjoCtVG4pzjH5+u5ZFpa6s81ywpjr0FJTxzbX/O7dXWg+pEJFLoCtUwY2b85ryePP7j\nqrMQ7i0oAeBnzy1g1bbcxi5NRHxI4d7ILjqpPWsfuIi+HZtX+/zwR2Yya212I1clIn6jbhkPOefo\ncvvUGp+/oHdbJo2u9duXiEQRdctEADPjg1uG1Pj8xyu2k51X2IgViYhfKNw9dkL7VGbeNqzG5/vf\nP430cVP4+rvdjViViEQ6hXsY6NSyKZN+choL/3ge3dtUneUJ4PJ/zeYPby5he24B4ycv55lZ3/Lu\nN9XOZigioj73cDQ/cxc/fGJOnbZdc/9FJMTpb7RItFCfewQ7qUNzzuzaklsvPL7Wbc988NNGqEhE\nIo3CPQwlxcfy2g0DGTOsO6vvH37YbXftK+KFOZnMXb+TrTn5jVOgiIQ9hXuYS4yL5X9O7XjYbe5+\nbzmjJs3lkkcDMz+NmjSHx6ZnNEZ5IhKmFO4R4L7L+tRpu+y8Im58cQFz1+/irx+tZtnmnAauTETC\nlcI9AjRNiCtfzpwwkmsHpde47UfLt5cvX/zoLKYu3dqQpYlImNJomQixdFPgKPyk4G0Laru6taKX\nrzuDHz81j0/GDmXO+p18ujKL539WdTIREQl/dR0to3CPYM45iksdPf/4wRG/NnPCyAaoSEQamoZC\nRgEzIyEuhswJI/nt+T2P6LVz1mnCbhE/U7j7xK+GdafPsal13v6qJ+fy0twNDViRiHhJ3TI+88aC\njdz25hIu7NO20snVmrRKTmDnviJSk+J46prTyc0v5rzemjBEJFypzz1KOefI3LmfLq2T2bR7P2f9\n3/Qjfg/1x4uEL/W5Rykzo0vrZAA6HtOU6b8/B4DB3VvV+T3GvPI14ycvJye/mPGTl/PFmh18sqL2\nbwEiEj505B4FduYV0jI5gR17C1m2JYeMrDz+PHVVra/r3T6VFVsPTvv36vVnMrBb3f9IiEjo6chd\nyrVKScTMaJOaxLm92nLD0G51el3FYIfASdjqfL46i/1FJUddp4iEjsI9Sq28dzgzbh3GP0adQsdj\nmtT5dS/P20BWbgHFpWVkZOXx3qLNXPvsfG57c0kDVisiR0rdMgJA+rgpR/X6Xu2a8eFvhoaoGhGp\nibpl5IjMuHUYJ3ao+zj5Q63atrfKuszsfXyZkX00ZYlIPenIXcoVFJeyLaeA9NbJFJaUct/7K3hp\n7ndH9B6Du7figctOIr11cvm3AQ2tFAmdkB65m9lwM1ttZhlmNq6a588xsxwzWxT8ubs+RYu3kuJj\nSQ8Oo0yMi+X+y07ipZ+fcUTv8WXGTs556HM+X51Vvs45x+yMbLw6kBCJRrWGu5nFAo8BFwG9gavM\nrHc1m850zp0S/Lk3xHWKR87q0ZrxlwR2d6vkhDq/7tpn55cvX/rYl/zoqXlMXbqNLXvyGfPy1xQU\nl4a8VhE5KK72TRgAZDjn1gOY2WvApcCKhixMwsfogekAjBrQmf8s2Mhd7y0/otcvCd6ueMwrX5ev\na5WSwFff7uLl686gVUpiyGoVkYC6dMt0ADZWeLwpuO5Qg8xsiZl9YGZ1mzpIIkJMjHHt4C4kxcfy\nk4HpZE4Yyar7qs7tet1ZXer8ni/M2cCqbXt5fnYm6eOm8OpXR9a3LyKHV5cj97r4GujsnMszsxHA\nu0CPQzcysxuAGwA6d+4coo8WLyTFx5I5YSRFJWXc9NJCrh2czpAeafzg1A6MnDirzu8z8bPAXK+3\nv72UUad34u73ltM2NZHd+4v5Qb8OnNghMDnJhp37KC1zdE1LaZD2iPhNraNlzGwgMN45d2Hw8e0A\nzrkHD/OaTKC/c67GcXAaLeNfn67czhdrdvDCnKO/pXDmhJGs2pbL8Edmlj8WiWahHC0zH+hhZl3M\nLAEYBUw+5MPamZkFlwcE31ezQUSp753QlnsvPTEk71VQXFoe7ABPz/q2yjY78wpZuGE3AHv2F/H8\n7EyNzJGoV2u3jHOuxMx+BXwExALPOOeWm9kvgs8/AVwB3GRmJUA+MMrptyvqrbpvOGYw+umvSE6M\n4ycDjyM3v5gL+7Sj110f1uk9bj3ktgb3vb+Cnw5KZ8XWXGauzebMri25/PHZOAfP/vR0Xp77HdNW\nbqdf5xac3LFFQzRLJCLoIibxxPjJy3ludma9XnturzZ8tirrsNu8ddMgTjvumPLPyZwwkoLiUuJi\njLhYXZgtkUu3H5Cwds8lvWnTrH5DIGsLdoDNe/JZsSW30h+QXnd9SO+7P6rXZ4pEGoW7eMLM+PA3\nQ0k7JOCT4kPzv+SvX/2GERMP9tWPfX0RAEWlZewrLKGsLPCNdcwrX/PG/I1VXv/+ki1c/dS8kNQi\n4oVQDYUUOWItkxO479IT+cVLC3n0qn70bNuMNs0Sydy5j8Ub9zD+vwevk4uLMUrK6t+F+M43m8uX\n+9zzEWPP68kt5/VgypKtTFmylStP71Rp+1+98g0QuHVCcKyASERRuIunhp/YrsrwxmOSE9iTX1xp\n3bI/XchTM9fz0MdrQvK5f5+2hiv6dyx/fNubizmpQ3O6pqWwdvvBO1wWlzoS4hTuEnl0QlXCknOO\naSuzOO24Y4iLNVKT4gG48O8zGH5iO0rLHP+cntHgdSwdfwHNgp8tEg50QlUimplxfu+2tExOKA92\ngI/GDmXs+T258eyu5esW330Bp6cf0yB1PDMrcHuE8x/+gtvfXlrtNs45Js1Yx8Zd+1kavI+OiNd0\n5C4R66GPVrNq216euiZwEOOc440FG+mWlkKrlESGPfR5g3zurD8Mo0OLJjgXuO/O5MVb+PWr35Q/\n//YvB9G+eRLbcws5pZPG2kto1fXIXeEuvnXf+yuqvaI1lN4dM5jLHvuy0rpOLZuwPaeQotIy5t95\nXpURQSJHQ90yEvXGnt+TH57WkcV3X1DluTtHnBCSzzg02AE27sqnqLQMgNMfmMakGesAyMjKI33c\nFL75bndIPlvkcBTu4lspiXH89Yd9ad40nswJIyuNyrl+aFcGdGnZKHX8eeoq7nhnKY9+thaAa575\nCuccj03PYMue/Erb5heV8u8v1lES/OMgUl/qlpGosnrbXmJjjO5tDt46OCMrj9yCYrq3SSE1Kb58\n7tfG9sEtQ3hl3ne8OHcDD/2wL1ec1pEVW3IZMXEmPdqkcHbPNK48vRM92zbzpD4JD+pzF6mnyYu3\n8M/P1nLVgM6cd0Jb/vTfFUxbub1Razire2teuu6Mav/QLL7nApo3OfzwzE2799M0IY6WRzA1okQG\nhbtIiM1bv5P/nTS32ueuGtC50WaT6to6mfP7tOXfX6zns9+dTde0FNbvyKN98yY0SYgFIH3cFOJj\njbUPjKj2Pf7w5hI6t2rKmGHdASgtc3y0fBvD+7QjJiZw0dYLczLp1S610bqvpG7qGu66QlWkjs7o\n2ooFfzyPWWuzyS8urTTu/d5L+7AuK4+vMnc1eB3rs/fx7y/WA3Du377g5I7Ny+epveeS3jz7ZSYQ\nuLp2zrqdLN+SQ2FJGZ+tyuKtmwYB8PqCwP10Oh7ThH9NX8e1g9O5/e2ljD2vJxf3bU+3tBTuDs6V\ne7gJUnL2F1NYWkqbZkkN1VypJx25ixyFrNwCmibGkZIYOE7K2lvAqq17adE0njvfWcbSzeF1UdPs\nceeyfsc+rn768DdFmz3uXAZN+AyAJ64+jdOOO4ak+JgqV+uecNeH5BeXaoasRqRuGRGPlZU5ypwj\nr7CEF+ds4G+fBO6L0yo5gZ37ijyurn46tGjCuIt6cUnfYwHKzwm8ceNArvz3HN66aSCnHVdzN07W\n3gId5R8lhbtIGCstc5SUlXHrf5bQNjWRJ2cevNiqU8smbNyVf5hXh4eRJ7dnypKtAPTt2JzFwa6h\n2y/qxY1nd8M5R5mD2GAf/psLN/H7/yzmvTGD6asrd+tNFzGJhLHYGCMxLpaJV/XjzpG9eTp4C4Uh\nPVrzynVnAoELrT773dm8fsOZ3HVxb/p1Dq9APBDsQHmwAzz4wSp25hUy8dMMut0xlY279rM9t4CH\nP14NwLxvd5KRlcfY1xexeU/VP2JvLNjIL15cyIw1O1gW7Nb6ZMV2dkXotx2v6MhdJAyVlTnMqHIv\n+YLiUhJiY1iyOYec/GJ6t0/l9reXMG1lYHaqY5snsSWnwIuSK4mPNYpLa8+WawelM/77ffjv4i1s\n2p1PWrNEfv+fxZW2mXD5SYwLnrzOnDCSPfuLaNG0bkM8Z6/LZm9BCRf2aVfjNtl5hWzZk8/JHVtQ\nWuZ4fnYmPzqjM0nxgZFHL87dwF3vLmPVfcPL13lJo2VEItiB4YiHOhAuFW9I9uTo/jw181su69eB\n1CZxzF63kz7HppKcEEefe7yZVrAuwQ7w3OxMEuNi+PeM9TVuM67CqKRrnvmKL9bsAGDseT25+dzu\nlDpHfGwM33y3m+NaJdMsKY4Hpqxk174iJi/eAsDKe4dTVFJG86bxbMsp4G8fr+b3Fx5P29QkLp44\ni225BWROGMlbCzdx7/sryMkv5lfndmfFllweCZ4r2VtQEhbhXlc6chfxuZz9xdz+zhKmLt3GJX2P\npW/H5jw3O5O/XtGXq54MjNufeFU/Fm/c0+A3WmsoFWfq+ung9PLhoIf60/f7cM/kwBDPhLgY+nZs\nzvzMwL1+MieM5KmZ67l/ykpGDzyOKUu2Vjrx/ehV/bigT1uy84pol5rEoo27+Z/H5zD5V4M5uWPj\ndZnphKqI1Or1+d8x79tdPHzlKeXr7nhnKa/Mq3xB1hNXn8bOfYX07diCix+d1dhlNorHfnQqW3Py\nuX/Kyhq36d0+lRVbc/nlOd34btd+3g+edxh2fBrXDenK4O6tG7xOhbuI1Jtzji63TwVgULdWvHL9\nmeXPzVizg6dnfVvePfLU6P5c90Lgd3lAekv2FZWwfEsuV/bvyBsLNlX7/n//376MfX1xtc9FstYp\nCaQkxvH5rcN4ce4G7vvvCt4dM5j01k3547vLuGPECbROObpbQCvcReSo5BeVUupc+QVah3rtq+8Y\n9/ZS5t7+PWIscJ7g0OBas30vpWWOi/4xs3zdBb3bMml0f0rLHN3umNqgbQgX553Qlmkrt9MsKY5X\nrz+TPsem1nvidQ2FFJGj0iQhtsZgBxg1oDPfPjiCds2TaJOaVO0Rac+2zTihfSq926cyeuBxTP/9\nOUwaHcil2GpOGo8eeByz/jCs/HFyQiw3ndMtBK3x1oEbz+0tKOHiR2cxZenWWl5x9DRaRkTqra5H\nn1NvGVLt+md/ejrd01JolhS4hUNcbOB4s2VyArExxvw7zwMC3T3tWyTxxvxNDOnRmuZN47n8X7N5\n5tr+JCfE8c43m3lt/kZ6tWvGqm17y9//jRsHcvd7yyqtq6shPVozc232Eb+uLlZsyeXik49tkPc+\nQN0yIhJ2SoMjX6o7uq/JgSybtjKLeet38qMzOtM1LYXcgmLufGcZN5/bnX2FJfzgX7Orff0dI3rx\n56mrAGiXmsTcO77XYPf2/+U53bhteK96vVZ97iIi1Vi1LZfSMsestdk8+MEqTu7YnMm/OovCklKO\n/+OHAMy4dRidWzWlsKSU7TmFvDRvA5NmrOerO7/Hdzv3M311Fo9NX1enz/t+32PLx9sfcOeIE7h+\naNd61a9wFxGpxd6CYhLiYkiMC1yclJG1l47HNK3TxUoHjurX/XkExaVlmFH+PpnZ+ziuVVOcC5xo\nrvgN4KZzuvGHeh61g65QFRGp1aG3MO7epu5TGH4ydihmRmyMERtT+Y9BeutkAA6cknj22tPZV1TC\nyJPa13uUzJFSuIuI1EOPI5jLdlivNg1YSfU0FFJExIcU7iIiPqRwFxHxIYW7iIgPKdxFRHxI4S4i\n4kMKdxERH1K4i4j4kGe3HzCzHcCGer68NdAwt2trfGpLePJLW/zSDlBbDjjOOZdW20aehfvRMLMF\ndbm3QiRQW8KTX9ril3aA2nKk1C0jIuJDCncRER+K1HCf5HUBIaS2hCe/tMUv7QC15YhEZJ+7iIgc\nXqQeuYuIyGFEXLib2XAzW21mGWY2zut6amNmmWa21MwWmdmC4LqWZvaJma0N/ntMhe1vD7ZttZld\n6F3lYGbPmFmWmS2rsO6Iazez04L/DTLMbKI11mwFtbdlvJltDu6bRWY2ItzbYmadzGy6ma0ws+Vm\ndktwfcTtl8O0JRL3S5KZfWVmi4Nt+VNwvXf7xTkXMT9ALLAO6AokAIuB3l7XVUvNmUDrQ9b9BRgX\nXB4H/F9wuXewTYlAl2BbYz2sfShwKrDsaGoHvgLOBAz4ALgoTNoyHvh9NduGbVuA9sCpweVmwJpg\nvRG3Xw7TlkjcLwakBJfjgXnBejzbL5F25D4AyHDOrXfOFQGvAZd6XFN9XAo8H1x+HriswvrXnHOF\nzrlvgQwCbfaEc24GsOuQ1UdUu5m1B1Kdc3Nd4P/cFyq8ptHU0JaahG1bnHNbnXNfB5f3AiuBDkTg\nfjlMW2oSzm1xzrm84MP44I/Dw/0SaeHeAdhY4fEmDv8/QzhwwDQzW2hmNwTXtXXObQ0ubwPaBpcj\noX1HWnv1V+jvAAACCklEQVSH4PKh68PFzWa2JNhtc+Arc0S0xczSgX4EjhIjer8c0haIwP1iZrFm\ntgjIAj5xznm6XyIt3CPRWc65U4CLgDFmNrTik8G/zhE5ZCmSaw96nEAX3ynAVuBv3pZTd2aWArwF\n/MY5l1vxuUjbL9W0JSL3i3OuNPi73pHAUfiJhzzfqPsl0sJ9M9CpwuOOwXVhyzm3OfhvFvAOgW6W\n7cGvXwT/zQpuHgntO9LaNweXD13vOefc9uAvZBnwJAe7wMK6LWYWTyAMX3bOvR1cHZH7pbq2ROp+\nOcA5tweYDgzHw/0SaeE+H+hhZl3MLAEYBUz2uKYamVmymTU7sAxcACwjUPM1wc2uAd4LLk8GRplZ\nopl1AXoQOLkSTo6o9uBX0lwzOzN41n90hdd46sAvXdAPCOwbCOO2BD/3aWClc+7hCk9F3H6pqS0R\nul/SzKxFcLkJcD6wCi/3S2OeUQ7FDzCCwFn1dcCdXtdTS61dCZwRXwwsP1Av0Ar4FFgLTANaVnjN\nncG2rcaDUSWH1P8qga/FxQT6/n5en9qB/gR+QdcB/yR48VwYtOVFYCmwJPjL1j7c2wKcReCr/RJg\nUfBnRCTul8O0JRL3y8nAN8GalwF3B9d7tl90haqIiA9FWreMiIjUgcJdRMSHFO4iIj6kcBcR8SGF\nu4iIDyncRUR8SOEuIuJDCncRER/6f+Fzv2V9H5/KAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1291d2c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
