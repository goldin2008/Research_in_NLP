{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练 RNN 语言模型\n",
    "\n",
    "1. 词语的embedding\n",
    "2. embedding空间和state空间\n",
    "3. 连续词语之间的关联"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from model import CharRNNLM\n",
    "from utils import VocabularyLoader, batche2string\n",
    "\n",
    "TF_VERSION = int(tf.__version__.split('.')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "    \n",
    "    def __init__(self, tensor_in, tensor_out, batch_size, seq_length):\n",
    "        \"\"\"初始化batch产生器\n",
    "        Input:\n",
    "            batch_size: 每一个mini-batch里面有多少样本。\n",
    "            seq_length: 每一个样本的长度，和batch_size一起决定了每个minibatch的数据量。\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        self.tensor_in = tensor_in\n",
    "        self.tensor_out = tensor_out\n",
    "        \n",
    "        self.create_batches()\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0\n",
    "\n",
    "    def create_batches(self):\n",
    "        self.num_batches = int(self.tensor_in.size / (self.batch_size * self.seq_length))\n",
    "        self.tensor_in = self.tensor_in[:self.num_batches * self.batch_size * self.seq_length]\n",
    "        self.tensor_out = self.tensor_out[:self.num_batches * self.batch_size * self.seq_length]\n",
    "        \n",
    "        # When the data (tesor) is too small, let's give them a better error message\n",
    "        if self.num_batches==0:\n",
    "            assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
    "        \n",
    "        self.x_batches = np.split(self.tensor_in.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "        self.y_batches = np.split(self.tensor_out.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "\n",
    "    def next_batch(self):\n",
    "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x, y\n",
    "\n",
    "class CopyBatchGenerator(BatchGenerator):\n",
    "    \n",
    "    def __init__(self, data, batch_size, seq_length):\n",
    "        \"\"\"初始化batch产生器\n",
    "        Input:\n",
    "            batch_size: 每一个mini-batch里面有多少样本。\n",
    "            seq_length: 每一个样本的长度，和batch_size一起决定了每个minibatch的数据量。\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        tensor_in = np.array(data)\n",
    "        tensor_out = np.copy(tensor_in)\n",
    "        tensor_out[:-1] = tensor_in[1:]\n",
    "        tensor_out[-1] = tensor_in[0]\n",
    "        \n",
    "        super(CopyBatchGenerator, self).__init__(tensor_in, tensor_out, batch_size, seq_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备阶段\n",
    "\n",
    "#### 1. 使用python的argparse工具 集中处理超参数\n",
    "\n",
    "* `argparse`用法比较直观，相对晦涩一些的语法是 **`action = store_true`**\n",
    "\n",
    "```python\n",
    "logging_args.add_argument('--debug', dest='debug',\n",
    "            action='store_true',\n",
    "            help='show debug information')\n",
    "```\n",
    "* 当模型比较复杂的时候，使用add_argument_group()将各种options分组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "def config_train(args=''):\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyper-parameters to configure the datasets.\n",
    "    # 数据相关的超参数\n",
    "    data_args = parser.add_argument_group('Dataset Options')\n",
    "    data_args.add_argument('--data_file', type = str,\n",
    "            default = 'data/ice_and_fire_zh/ice_and_fire_utf8.txt',\n",
    "            help = 'data file')\n",
    "    data_args.add_argument('--encoding', type = str, default = 'utf-8',\n",
    "            help = 'the encoding format of data file.')\n",
    "    data_args.add_argument('--num_unrollings', type=int, default=20,\n",
    "            help='number of unrolling steps.')\n",
    "    data_args.add_argument('--train_frac', type = float, default=0.9,\n",
    "            help='fraction of data used for training.')\n",
    "    data_args.add_argument('--valid_frac', type=float, default=0.05,\n",
    "            help='fraction of data used for validation.')\n",
    "    # test_frac is computed as (1 - train_frac - valid_frac).\n",
    "\n",
    "\n",
    "    # hyper-parameters to configure the neural network.\n",
    "    # 模型结构相关的超参数\n",
    "    network_args = parser.add_argument_group('Model Arch Options')\n",
    "    network_args.add_argument('--embedding_size', type=int, default=128,\n",
    "            help='size of character embeddings, 0 for one-hot')\n",
    "    network_args.add_argument('--hidden_size', type=int, default=256,\n",
    "            help='size of RNN hidden state vector')\n",
    "    network_args.add_argument('--cell_type', type=str, default='lstm',\n",
    "            help='which RNN cell to use (rnn, lstm or gru).')\n",
    "    network_args.add_argument('--num_layers', type=int, default=2,\n",
    "            help='number of layers in the RNN')\n",
    "\n",
    "\n",
    "    # hyper-parameters to control the training.\n",
    "    # 训练和优化相关的超参数\n",
    "    training_args = parser.add_argument_group('Model Training Options')\n",
    "    # 1. Parameters for iterating through samples\n",
    "    training_args.add_argument('--num_epochs', type = int, default=50,\n",
    "            help='number of epochs')\n",
    "    training_args.add_argument('--batch_size', type = int, default=20,\n",
    "            help='minibatch size')\n",
    "    # 2. Parameters for dropout setting.\n",
    "    training_args.add_argument('--dropout', type=float, default=0.0,\n",
    "            help='dropout rate, default to 0 (no dropout).')\n",
    "    training_args.add_argument('--input_dropout', type=float, default=0.0,\n",
    "            help=('dropout rate on input layer, default to 0 (no dropout),'\n",
    "                'and no dropout if using one-hot representation.'))\n",
    "    # 3. Parameters for gradient descent.\n",
    "    training_args.add_argument('--max_grad_norm', type=float, default=5.,\n",
    "            help='clip global grad norm')\n",
    "    training_args.add_argument('--learning_rate', type=float, default=5e-3,\n",
    "            help='initial learning rate')\n",
    "\n",
    "\n",
    "    # Parameters for manipulating logging and saving models.\n",
    "    # 学习日志和结果相关的超参数\n",
    "    logging_args = parser.add_argument_group('Logging Options')\n",
    "    # 1. Directory to output models and other records.\n",
    "    logging_args.add_argument('--output_dir', type = str,\n",
    "            default = 'demo_model',\n",
    "            help = ('directory to store final and'\n",
    "            ' intermediate results and models'))\n",
    "    # 2. Parameters for printing messages.\n",
    "    logging_args.add_argument('--progress_freq', type=int, default=100,\n",
    "            help=('frequency for progress report in training and evalution.'))\n",
    "    logging_args.add_argument('--verbose', type=int, default=0,\n",
    "            help=('whether to show progress report in training and evalution.'))\n",
    "    logging_args.add_argument('--debug', dest='debug',action='store_true',\n",
    "            help='show debug information')\n",
    "    logging_args.add_argument('--test', dest='test', action='store_true',\n",
    "            help=('parameter for unittesting. Use the first 1000 '\n",
    "                'character to as data to test the implementation'))\n",
    "    # 3. Parameters to feed in the initial model and current best model.\n",
    "    logging_args.add_argument('--init_model', type=str,\n",
    "            default='', help=('initial model'))\n",
    "    logging_args.add_argument('--best_model', type=str,\n",
    "            default='', help=('current best model'))\n",
    "    logging_args.add_argument('--best_valid_ppl', type=float,\n",
    "            default=np.Inf, help=('current valid perplexity'))\n",
    "    # 4. Parameters for using saved best models.\n",
    "    logging_args.add_argument('--init_dir', type=str, default='',\n",
    "            help='continue from the outputs in the given directory')\n",
    "    \n",
    "    args = parser.parse_args(args.split())\n",
    "    \n",
    "    return args\n",
    "\n",
    "\n",
    "def config_sample(args=''):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # hyper-parameters for using saved best models.\n",
    "    # 学习日志和结果相关的超参数\n",
    "    logging_args = parser.add_argument_group('Logging_Options')\n",
    "    logging_args.add_argument('--init_dir', type=str,\n",
    "            default='demo_model/',\n",
    "            help='continue from the outputs in the given directory')\n",
    "\n",
    "    # hyper-parameters for sampling.\n",
    "    # 设置sampling相关的超参数\n",
    "    testing_args = parser.add_argument_group('Sampling Options')\n",
    "    testing_args.add_argument('--max_prob', dest='max_prob', action='store_true',\n",
    "                        help='always pick the most probable next character in sampling')\n",
    "    testing_args.set_defaults(max_prob=False)\n",
    "\n",
    "    testing_args.add_argument('--start_text', type=str,\n",
    "                        default='The meaning of life is ',\n",
    "                        help='the text to start with')\n",
    "\n",
    "    testing_args.add_argument('--length', type=int,\n",
    "                        default=100,\n",
    "                        help='length of sampled sequence')\n",
    "\n",
    "    testing_args.add_argument('--seed', type=int,\n",
    "                        default=-1,\n",
    "                        help=('seed for sampling to replicate results, '\n",
    "                              'an integer between 0 and 4294967295.'))\n",
    "\n",
    "    args = parser.parse_args(args.split())\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 20,\n",
       " 'best_model': '',\n",
       " 'best_valid_ppl': inf,\n",
       " 'cell_type': 'lstm',\n",
       " 'data_file': 'data/ice_and_fire_zh/ice_and_fire_utf8.txt',\n",
       " 'debug': True,\n",
       " 'dropout': 0.0,\n",
       " 'embedding_size': 128,\n",
       " 'encoding': 'utf-8',\n",
       " 'hidden_size': 256,\n",
       " 'init_dir': '',\n",
       " 'init_model': '',\n",
       " 'input_dropout': 0.0,\n",
       " 'learning_rate': 0.005,\n",
       " 'max_grad_norm': 5.0,\n",
       " 'num_epochs': 50,\n",
       " 'num_layers': 2,\n",
       " 'num_unrollings': 20,\n",
       " 'output_dir': 'demo_model',\n",
       " 'progress_freq': 100,\n",
       " 'test': False,\n",
       " 'train_frac': 0.9,\n",
       " 'valid_frac': 0.05,\n",
       " 'verbose': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = config_train('--debug --verbose 1')\n",
    "vars(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备阶段\n",
    "\n",
    "#### 2. 处理文件地址，预备 模型的记录，保存，读取操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "All final and intermediate outputs will be stored in demo_model/\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 在目标文件夹（output_dir)里面创建\n",
    "# save_model, best_model 和 tensorboar_log\n",
    "# 分别保存\n",
    "#   1. 训练练过程中的中间模型\n",
    "#   2. 目前最好的模型\n",
    "#   3. 用于tensorboard可视化的日志文件\n",
    "\n",
    "args.save_model = os.path.join(args.output_dir, 'save_model/model')\n",
    "args.save_best_model = os.path.join(args.output_dir, 'best_model/model')\n",
    "args.tb_log_dir = os.path.join(args.output_dir, 'tensorboard_log/')\n",
    "args.vocab_file = ''\n",
    "\n",
    "# 小心使用，如果目标文件夹已经存在，先将其删除\n",
    "print('=' * 80)\n",
    "print('All final and intermediate outputs will be stored in %s/' % \n",
    "      args.output_dir)\n",
    "print('=' * 80 + '\\n')\n",
    "if os.path.exists(args.output_dir):\n",
    "    import shutil\n",
    "    shutil.rmtree(args.output_dir)\n",
    "\n",
    "# 建立目标文件夹和子文件夹\n",
    "for paths in [args.save_model, args.save_best_model, args.tb_log_dir]:\n",
    "    os.makedirs(os.path.dirname(paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型将保存在：demo_model/save_model/model\n",
      "最好的模型将保存在：demo_model/best_model/model\n",
      "tensorboard相关文件将保存在：demo_model/tensorboard_log/\n"
     ]
    }
   ],
   "source": [
    "print('模型将保存在：%s' % args.save_model)\n",
    "print('最好的模型将保存在：%s' % args.save_best_model)\n",
    "print('tensorboard相关文件将保存在：%s' % args.tb_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'logging' from '/Users/yuleinku/anaconda/lib/python3.6/logging/__init__.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(stream=sys.stdout,\n",
    "                    format='%(asctime)s %(levelname)s:%(message)s',\n",
    "                    level=logging.INFO, datefmt='%I:%M:%S')\n",
    "logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:57:50 INFO:args are:\n",
      "Namespace(batch_size=20, best_model='', best_valid_ppl=inf, cell_type='lstm', data_file='data/ice_and_fire_zh/ice_and_fire_utf8.txt', debug=True, dropout=0.0, embedding_size=128, encoding='utf-8', hidden_size=256, init_dir='', init_model='', input_dropout=0.0, learning_rate=0.005, max_grad_norm=5.0, num_epochs=50, num_layers=2, num_unrollings=20, output_dir='demo_model', progress_freq=100, save_best_model='demo_model/best_model/model', save_model='demo_model/save_model/model', tb_log_dir='demo_model/tensorboard_log/', test=False, train_frac=0.9, valid_frac=0.05, verbose=1, vocab_file='')\n",
      "10:57:50 INFO:\n",
      "\n",
      "Parameters are:\n",
      "{\n",
      "    \"batch_size\": 20,\n",
      "    \"cell_type\": \"lstm\",\n",
      "    \"dropout\": 0.0,\n",
      "    \"embedding_size\": 128,\n",
      "    \"hidden_size\": 256,\n",
      "    \"input_dropout\": 0.0,\n",
      "    \"learning_rate\": 0.005,\n",
      "    \"max_grad_norm\": 5.0,\n",
      "    \"num_layers\": 2,\n",
      "    \"num_unrollings\": 20\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if args.debug:\n",
    "    logging.info('args are:\\n%s', args)\n",
    "\n",
    "if len(args.init_dir) != 0:\n",
    "    with open(os.path.join(args.init_dir, 'result.json'), 'r') as f:\n",
    "        result = json.load(f)\n",
    "        \n",
    "    params = result['params']\n",
    "    args.init_model = result['latest_model']\n",
    "    best_model = result['best_model']\n",
    "    best_valid_ppl = result['best_valid_ppl']\n",
    "    \n",
    "    if 'encoding' in result:\n",
    "        args.encoding = result['encoding']\n",
    "    else:\n",
    "        args.encoding = 'utf-8'\n",
    "    args.vocab_file = os.path.join(args.init_dir, 'vocab.json')\n",
    "    \n",
    "else:\n",
    "    params = {'batch_size': args.batch_size,\n",
    "              'num_unrollings': args.num_unrollings,\n",
    "              'hidden_size': args.hidden_size,\n",
    "              'max_grad_norm': args.max_grad_norm,\n",
    "              'embedding_size': args.embedding_size,\n",
    "              'num_layers': args.num_layers,\n",
    "              'learning_rate': args.learning_rate,\n",
    "              'cell_type': args.cell_type,\n",
    "              'dropout': args.dropout,\n",
    "              'input_dropout': args.input_dropout}\n",
    "    best_model = ''\n",
    "\n",
    "import json\n",
    "logging.info('\\n\\nParameters are:\\n%s\\n',\n",
    "             json.dumps(params, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "params = {'batch_size': args.batch_size,\n",
    "          'num_unrollings': args.num_unrollings,\n",
    "          'hidden_size': args.hidden_size,\n",
    "          'max_grad_norm': args.max_grad_norm,\n",
    "          'embedding_size': args.embedding_size,\n",
    "          'num_layers': args.num_layers,\n",
    "          'learning_rate': args.learning_rate,\n",
    "          'cell_type': args.cell_type,\n",
    "          'dropout': args.dropout,\n",
    "          'input_dropout': args.input_dropout}\n",
    "s_params= json.dumps(params, sort_keys=True, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/ice_and_fire_zh/ice_and_fire_utf8.txt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:57:54 INFO:Reading data from: data/ice_and_fire_zh/ice_and_fire_utf8.txt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/ice_and_fire_zh/ice_and_fire_utf8.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-100b54cf3065>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Read and split data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reading data from: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yuleinku/anaconda/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, encoding, errors, buffering)\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0;31m# Force opening of the file in binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/ice_and_fire_zh/ice_and_fire_utf8.txt'"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "# codecs: python的编码和解码模块, 可以参考\n",
    "# http://blog.csdn.net/iamaiearner/article/details/9138865\n",
    "# http://www.cnblogs.com/TsengYuen/archive/2012/05/22/2513290.html\n",
    "# http://blog.csdn.net/suofiya2008/article/details/5579413\n",
    "# 等博客\n",
    "\n",
    "# Read and split data.\n",
    "logging.info('Reading data from: %s', args.data_file)\n",
    "with codecs.open(args.data_file, encoding=args.encoding) as f:\n",
    "    text = f.read()\n",
    "\n",
    "if args.test:\n",
    "    text = text[:50000]\n",
    "logging.info('Number of characters: %s', len(text))\n",
    "\n",
    "if args.debug:\n",
    "    logging.info('First %d characters: %s', 10, text[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将整本数据集分割成train, test, validation数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:57:56 INFO:Creating train, valid, test split\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-021e89a33b3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Creating train, valid, test split'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_frac\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvalid_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_frac\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalid_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "logging.info('Creating train, valid, test split')\n",
    "train_size = int(args.train_frac * len(text))\n",
    "valid_size = int(args.valid_frac * len(text))\n",
    "test_size = len(text) - train_size - valid_size\n",
    "train_text = text[:train_size]\n",
    "valid_text = text[train_size:train_size + valid_size]\n",
    "test_text = text[train_size + valid_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:57:58 INFO:Creating vocabulary\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e6cb4a74cfd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Creating vocabulary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mvocab_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mvocab_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vocab.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mvocab_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "vocab_loader = VocabularyLoader()\n",
    "if len(args.vocab_file) != 0:\n",
    "    vocab_loader.load_vocab(args.vocab_file, args.encoding)\n",
    "else:\n",
    "    logging.info('Creating vocabulary')\n",
    "    vocab_loader.create_vocab(text)\n",
    "    vocab_file = os.path.join(args.output_dir, 'vocab.json')\n",
    "    vocab_loader.save_vocab(vocab_file, args.encoding)\n",
    "    logging.info('Vocabulary is saved in %s', vocab_file)\n",
    "    args.vocab_file = vocab_file\n",
    "\n",
    "params['vocab_size'] = vocab_loader.vocab_size\n",
    "logging.info('Vocab size: %d\\n', vocab_loader.vocab_size)\n",
    "\n",
    "logging.info('sampled words in vocabulary %s ' % list(zip(list(vocab_loader.index_vocab_dict.keys())[:1000:100], \n",
    "         list(vocab_loader.index_vocab_dict.values())[:1000:100])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VocabularyLoader' object has no attribute 'vocab_index_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-564a3f67cabb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_unrollings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_unrollings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCopyBatchGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_index_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_unrollings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mvalid_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCopyBatchGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_index_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_unrollings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCopyBatchGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_index_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_unrollings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VocabularyLoader' object has no attribute 'vocab_index_dict'"
     ]
    }
   ],
   "source": [
    "# Create batch generators.\n",
    "batch_size = params['batch_size']\n",
    "num_unrollings = params['num_unrollings']\n",
    "\n",
    "train_batches = CopyBatchGenerator(list(map(vocab_loader.vocab_index_dict.get, train_text)), batch_size, num_unrollings)\n",
    "valid_batches = CopyBatchGenerator(list(map(vocab_loader.vocab_index_dict.get, valid_text)), batch_size, num_unrollings)\n",
    "test_batches = CopyBatchGenerator(list(map(vocab_loader.vocab_index_dict.get, test_text)), batch_size, num_unrollings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一个训练样本是这个样子的：\n",
    "* input： 《冰与火之歌》全集（1-5卷）【实体书精\n",
    "* output：  冰与火之歌》全集（1-5卷）【实体书精校\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:58:02 INFO:Test the batch generators\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_batches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-01e7a3f55173>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test the batch generators'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatche2string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_vocab_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatche2string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_vocab_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_batches' is not defined"
     ]
    }
   ],
   "source": [
    "logging.info('Test the batch generators')\n",
    "x, y = train_batches.next_batch()\n",
    "print(x[0])\n",
    "logging.info((str(x[0]), str(batche2string(x[0], vocab_loader.index_vocab_dict))))\n",
    "logging.info((str(y[0]), str(batche2string(y[0], vocab_loader.index_vocab_dict))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:58:03 INFO:Creating graph\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'vocab_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e9a3aaef6cc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtrain_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCharRNNLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'vocab_size'"
     ]
    }
   ],
   "source": [
    "# 建立训练，valid，测试 对象\n",
    "logging.info('Creating graph')\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope('training'):\n",
    "        train_model = CharRNNLM(is_training=True, infer=False, **params)\n",
    "    \n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    with tf.name_scope('validation'):\n",
    "        valid_model = CharRNNLM(is_training=False, infer=False, **params)\n",
    "    \n",
    "    with tf.name_scope('evaluation'):\n",
    "        test_model = CharRNNLM(is_training=False, infer=False, **params)\n",
    "        saver = tf.train.Saver(name='model_saver')\n",
    "        best_model_saver = tf.train.Saver(name='best_model_saver')\n",
    "\n",
    "logging.info('Start training\\n')\n",
    "\n",
    "result = {}\n",
    "result['params'] = params\n",
    "result['vocab_file'] = args.vocab_file\n",
    "result['encoding'] = args.encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop: ========================\n",
    "\n",
    "    Training: ----------------------------\n",
    "        + One Epoch Training Data\n",
    "        + Update Parameters   \n",
    "        + Save Updated Parameters\n",
    "    Evaluation: ----------------------------\n",
    "        + One Epoch Validation/Heldout Data\n",
    "        + Evaluate Performance of Parameters\n",
    "        + Update and Save Best Parameters, if New Best\n",
    "\n",
    "Test: ----------------------------\n",
    "    + Restore Best Model\n",
    "    + One Epoch Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n",
      "10:58:05 WARNING:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n",
      "WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n",
      "10:58:05 WARNING:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n",
      "10:58:05 INFO:=================== Epoch 0 ===================\n",
      "\n",
      "10:58:05 INFO:Training on training set\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-9e36a3ee4ac5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# training step, running one epoch on training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             ppl, train_summary_str, global_step = train_model.run_epoch(\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_model' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-9e36a3ee4ac5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        graph_info = session.graph_def\n",
    "        \n",
    "        train_writer = tf.summary.FileWriter(\n",
    "            args.tb_log_dir + 'train/', graph_info)\n",
    "        valid_writer = tf.summary.FileWriter(\n",
    "            args.tb_log_dir + 'valid/', graph_info)\n",
    "        \n",
    "        # load a saved model or start from random initialization.\n",
    "        if len(args.init_model) != 0:\n",
    "            saver.restore(session, args.init_model)\n",
    "        else:\n",
    "            tf.global_variables_initializer().run()\n",
    "\n",
    "        learning_rate = args.learning_rate\n",
    "        for epoch in range(args.num_epochs):\n",
    "            logging.info('=' * 19 + ' Epoch %d ' + '=' * 19 + '\\n',epoch)\n",
    "            logging.info('Training on training set')\n",
    "            \n",
    "            ## 第一部分.\n",
    "            # training step, running one epoch on training data\n",
    "            ppl, train_summary_str, global_step = train_model.run_epoch(\n",
    "                session, train_batches, is_training=True,\n",
    "                learning_rate=learning_rate, verbose=args.verbose,\n",
    "                freq=args.progress_freq)\n",
    "            # record the summary\n",
    "            train_writer.add_summary(train_summary_str, global_step)\n",
    "            train_writer.flush()\n",
    "            # save model\n",
    "            # 注意：save操作在session内部才有意义！\n",
    "            saved_path = saver.save(session, args.save_model,\n",
    "                                    global_step=train_model.global_step)\n",
    "            logging.info('Latest model saved in %s\\n', saved_path)\n",
    "            \n",
    "            ## 第二部分.\n",
    "            # evaluation step, running one epoch on validation data\n",
    "            logging.info('Evaluate on validation set')\n",
    "            valid_ppl, valid_summary_str, _ = valid_model.run_epoch(\n",
    "                session, valid_batches, is_training=False,\n",
    "                learning_rate=learning_rate, verbose=args.verbose,\n",
    "                freq=args.progress_freq)\n",
    "            # save and update best model\n",
    "            if (len(best_model) == 0) or (valid_ppl < best_valid_ppl):\n",
    "                best_model = best_model_saver.save(\n",
    "                    session, args.save_best_model,\n",
    "                    global_step=train_model.global_step)\n",
    "                best_valid_ppl = valid_ppl\n",
    "            else:\n",
    "                learning_rate /= 2.0\n",
    "                logging.info('Decay the learning rate: ' + str(learning_rate))\n",
    "            valid_writer.add_summary(valid_summary_str, global_step)\n",
    "            valid_writer.flush()\n",
    "            logging.info('Best model is saved in %s', best_model)\n",
    "            logging.info('Best validation ppl is %f\\n', best_valid_ppl)\n",
    "\n",
    "            ## 第三部分.\n",
    "            # update readable summary\n",
    "            result['latest_model'] = saved_path\n",
    "            result['best_model'] = best_model\n",
    "            # Convert to float because numpy.float is not json serializable.\n",
    "            result['best_valid_ppl'] = float(best_valid_ppl)\n",
    "\n",
    "            result_path = os.path.join(args.output_dir, 'result.json')\n",
    "            if os.path.exists(result_path):\n",
    "                os.remove(result_path)\n",
    "            with open(result_path, 'w') as f:\n",
    "                json.dump(result, f, indent=2, sort_keys=True)\n",
    "\n",
    "        logging.info('Latest model is saved in %s', saved_path)\n",
    "        logging.info('Best model is saved in %s', best_model)\n",
    "        logging.info('Best validation ppl is %f\\n', best_valid_ppl)\n",
    "\n",
    "        logging.info('Evaluate the best model on test set')\n",
    "        saver.restore(session, best_model)\n",
    "        test_ppl, _, _ = test_model.run_epoch(session, test_batches, is_training=False,\n",
    "                                 learning_rate=learning_rate, verbose=args.verbose, freq=args.progress_freq)\n",
    "        result['test_ppl'] = float(test_ppl)\n",
    "finally:\n",
    "    result_path = os.path.join(args.output_dir, 'result.json')\n",
    "    if os.path.exists(result_path):\n",
    "        os.remove(result_path)\n",
    "    with open(result_path, 'w') as f:\n",
    "        json.dump(result, f, indent=2, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init_dir': 'demo_model',\n",
       " 'length': 100,\n",
       " 'max_prob': False,\n",
       " 'seed': -1,\n",
       " 'start_text': '之后'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = config_sample('--init_dir demo_model --length 100 --start_text 之后')\n",
    "vars(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-3f3aac9dc8ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Prepare parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'result.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'best_model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yuleinku/anaconda/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yuleinku/anaconda/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yuleinku/anaconda/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yuleinku/anaconda/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(stream=sys.stdout,\n",
    "                    format='%(asctime)s %(levelname)s:%(message)s', \n",
    "                    level=logging.INFO, datefmt='%I:%M:%S')\n",
    "\n",
    "# Prepare parameters.\n",
    "with open(os.path.join(args.init_dir, 'result.json'), 'r') as f:\n",
    "    result = json.load(f)\n",
    "params = result['params']\n",
    "best_model = result['best_model']\n",
    "best_valid_ppl = result['best_valid_ppl']\n",
    "if 'encoding' in result:\n",
    "    args.encoding = result['encoding']\n",
    "else:\n",
    "    args.encoding = 'utf-8'\n",
    "\n",
    "args.vocab_file = os.path.join(args.init_dir, 'vocab.json')\n",
    "vocab_loader = VocabularyLoader()\n",
    "vocab_loader.load_vocab(args.vocab_file, args.encoding)\n",
    "\n",
    "logging.info('best_model: %s\\n', best_model)\n",
    "\n",
    "# Create graphs\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope('evaluation'):\n",
    "        model = CharRNNLM(is_training=False, infer=True, **params)\n",
    "    saver = tf.train.Saver(name='model_saver')\n",
    "\n",
    "if args.seed >= 0:\n",
    "    np.random.seed(args.seed)\n",
    "with tf.Session(graph=graph) as session:\n",
    "    saver.restore(session, best_model)\n",
    "    sample = model.sample_seq(session, args.length, args.start_text, vocab_loader,\n",
    "                              max_prob=args.max_prob)\n",
    "    print('Sampled text is:\\n\\n%s' % sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
