{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sys.path.append('/home/dong/Dropbox/Projects/NLP/seq2seq')\n",
    "# sys.path.append('C:\\\\Users\\\\reade\\\\Documents\\\\lecture4\\\\seq2seq')\n",
    "sys.path.append('/Users/yuleinku/Google Drive/BOOK/聊天机器人Chatbot/lecture4/seq2seq')\n",
    "from seq2seq.encoders import rnn_encoder\n",
    "from seq2seq.decoders import basic_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 产生一个demo 合成数据minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "产生10个长度不一（最短3，最长8）的sequences, 其中前十个是:\n",
      "[7, 2, 8, 6, 3, 4, 6, 8]\n",
      "[7, 6, 2, 8, 9, 3]\n",
      "[7, 3, 7, 2, 7]\n",
      "[4, 9, 6, 4, 3, 7, 8]\n",
      "[9, 8, 3, 4]\n",
      "[5, 7, 3]\n",
      "[2, 3, 4, 5, 3, 9]\n",
      "[6, 4, 4, 6, 6, 7, 9]\n",
      "[4, 9, 3, 2, 5, 4]\n",
      "[6, 5, 4, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "\n",
    "vocab_size = 10\n",
    "input_embedding_size = 20\n",
    "encoder_hidden_units = 25\n",
    "\n",
    "decoder_hidden_units = encoder_hidden_units\n",
    "\n",
    "import helpers as data_helpers\n",
    "batch_size = 10\n",
    "\n",
    "# 一个generator，每次产生一个minibatch的随机样本\n",
    "\n",
    "batches = data_helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "print('产生%d个长度不一（最短3，最长8）的sequences, 其中前十个是:' % batch_size)\n",
    "for seq in next(batches)[:min(batch_size, 10)]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用seq2seq库实现seq2seq模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "mode = tf.contrib.learn.ModeKeys.TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 计算图的数据的placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('minibatch'):\n",
    "    encoder_inputs = tf.placeholder(shape=(None, None),\n",
    "                                    dtype=tf.int32,\n",
    "                                    name='encoder_inputs')\n",
    "    encoder_inputs_length = tf.placeholder(shape=(None,),\n",
    "                                           dtype=tf.int32,\n",
    "                                           name='encoder_inputs_length')\n",
    "\n",
    "    decoder_targets = tf.placeholder(shape=(None, None),\n",
    "                                     dtype=tf.int32,\n",
    "                                     name='decoder_targets')\n",
    "    \n",
    "    decoder_inputs = tf.placeholder(shape=(None, None),\n",
    "                                    dtype=tf.int32,\n",
    "                                    name='decoder_inputs')\n",
    "    decoder_inputs_length = tf.placeholder(shape=(None,),\n",
    "                                            dtype=tf.int32,\n",
    "                                            name='decoder_inputs_length')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 定义 encoding 模型，使用seq2seq.encoder \n",
    "\n",
    "#### 2-a. encoding过程的hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init_scale': 0.04,\n",
       " 'rnn_cell': {'cell_class': 'BasicLSTMCell',\n",
       "  'cell_params': {'num_units': 25},\n",
       "  'dropout_input_keep_prob': 1.0,\n",
       "  'dropout_output_keep_prob': 1.0,\n",
       "  'num_layers': 1,\n",
       "  'residual_combiner': 'add',\n",
       "  'residual_connections': False,\n",
       "  'residual_dense': False}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_params = rnn_encoder.UnidirectionalRNNEncoder.default_params()\n",
    "encoder_params[\"rnn_cell\"][\"cell_params\"][\"num_units\"] = encoder_hidden_units\n",
    "encoder_params[\"rnn_cell\"][\"cell_class\"] = \"BasicLSTMCell\"\n",
    "encoder_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-b. 定义encoding过程\n",
    "1. input\\_embedding\n",
    "2. UnidirectionalRNNEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. input embedding\n",
    "with tf.name_scope('embedding'):\n",
    "    input_embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0),\n",
    "        dtype=tf.float32)\n",
    "\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(input_embeddings, encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating UnidirectionalRNNEncoder in mode=train\n",
      "INFO:tensorflow:\n",
      "UnidirectionalRNNEncoder:\n",
      "  init_scale: 0.04\n",
      "  rnn_cell:\n",
      "    cell_class: BasicLSTMCell\n",
      "    cell_params: {num_units: 25}\n",
      "    dropout_input_keep_prob: 1.0\n",
      "    dropout_output_keep_prob: 1.0\n",
      "    num_layers: 1\n",
      "    residual_combiner: add\n",
      "    residual_connections: false\n",
      "    residual_dense: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. encoding with UnidirectionalRNNEncoder\n",
    "encode_fn = rnn_encoder.UnidirectionalRNNEncoder(encoder_params, mode)\n",
    "encoder_output = encode_fn(encoder_inputs_embedded, encoder_inputs_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 定义decoding模型，使用seq2seq.decoders\n",
    "1. input embedding\n",
    "2. helper <-- decoder_input, decoder_input_length\n",
    "3. basic_decoder.BasicDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init_scale': 0.04,\n",
       " 'max_decode_length': 16,\n",
       " 'rnn_cell': {'cell_class': 'BasicLSTMCell',\n",
       "  'cell_params': {'num_units': 25},\n",
       "  'dropout_input_keep_prob': 1.0,\n",
       "  'dropout_output_keep_prob': 1.0,\n",
       "  'num_layers': 1,\n",
       "  'residual_combiner': 'add',\n",
       "  'residual_connections': False,\n",
       "  'residual_dense': False}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_params = basic_decoder.BasicDecoder.default_params()\n",
    "decode_params[\"rnn_cell\"][\"cell_params\"][\"num_units\"] = decoder_hidden_units\n",
    "decode_params[\"max_decode_length\"] = 16\n",
    "decode_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_inputs_embedded = tf.nn.embedding_lookup(input_embeddings, decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from seq2seq.contrib.seq2seq import helper as decode_helper\n",
    "with tf.name_scope('minibatch'):\n",
    "    helper_ = decode_helper.TrainingHelper(\n",
    "        inputs = decoder_inputs_embedded,\n",
    "        sequence_length = decoder_inputs_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BasicDecoder in mode=train\n",
      "INFO:tensorflow:\n",
      "BasicDecoder:\n",
      "  init_scale: 0.04\n",
      "  max_decode_length: 16\n",
      "  rnn_cell:\n",
      "    cell_class: BasicLSTMCell\n",
      "    cell_params: {num_units: 25}\n",
      "    dropout_input_keep_prob: 1.0\n",
      "    dropout_output_keep_prob: 1.0\n",
      "    num_layers: 1\n",
      "    residual_combiner: add\n",
      "    residual_connections: false\n",
      "    residual_dense: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoder_fn = basic_decoder.BasicDecoder(params=decode_params,\n",
    "                                       mode=mode,\n",
    "                                       vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_output, decoder_state = decoder_fn(encoder_output.final_state, helper_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32), \n",
    "        logits=tf.transpose(decoder_output.logits, perm = [1, 0, 2]))\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# 通过阅读decoder_helper的定义，\n",
    "# 输入数据是batch-major\n",
    "# 而输出数据是time-major...\n",
    "# 所以需要对输出的logits做一次transpose\n",
    "# labels: [batch_size, max_length, vocab_size]\n",
    "# logits （tranpose之前）: [max_length, batch_size, vocab_size] \n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    logits = tf.transpose(decoder_output.logits, perm=[1,0,2]), labels = decoder_targets))\n",
    "\"\"\"\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    \n",
    "    encoder_inputs_, encoder_inputs_length_ = data_helpers.batch(batch)\n",
    "    decoder_targets_, _ = data_helpers.batch(\n",
    "        [(sequence) + [EOS] for sequence in batch]\n",
    "    )\n",
    "    decoder_inputs_, decoder_inputs_length_ = data_helpers.batch(\n",
    "        [[EOS] + (sequence) for sequence in batch]\n",
    "    )\n",
    "    \n",
    "    # 在feedDict里面，key可以是一个Tensor\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_.T,\n",
    "        decoder_inputs: decoder_inputs_.T,\n",
    "        decoder_targets: decoder_targets_.T,\n",
    "        encoder_inputs_length: encoder_inputs_length_,\n",
    "        decoder_inputs_length: decoder_inputs_length_\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 我们已经定义了一个计算图, 下面开始训练模型\n",
    "\n",
    "* 图的输入端是encoder_inputs 和 encoder_inputs_length\n",
    "* 图的输出端是encoder_output\n",
    "```python\n",
    "[encoder_out1, decoder_out1, loss] = sess.run(\n",
    "    [encoder_output, decoder_output, loss], fd)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "产生100个长度不一的sequence\n",
      "其中前十个是:\n",
      "[4, 3, 5, 7, 3]\n",
      "[5, 4, 2, 2, 8]\n",
      "[4, 8, 8, 6]\n",
      "[5, 7, 3, 6, 7, 8, 7, 9]\n",
      "[3, 3, 7]\n",
      "[8, 9, 5, 9, 2]\n",
      "[9, 6, 6, 8, 2, 5]\n",
      "[6, 7, 9, 3, 5, 3, 7, 7]\n",
      "[9, 4, 4, 6, 3, 7]\n",
      "[9, 8, 6]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "batches = data_helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                        vocab_lower=2, vocab_upper=10,\n",
    "                                        batch_size=batch_size)\n",
    "\n",
    "print('产生100个长度不一的sequence')\n",
    "print('其中前十个是:')\n",
    "for seq in next(batches)[:10]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 打印一个样本，检查数据正确与否"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_inputs:\n",
      "[6 5 4 6 9 0 0 0]\n",
      "encoder_inputs_length:\n",
      "5\n",
      "decoder_inputs:\n",
      "[1 6 5 4 6 9 0 0 0]\n",
      "decoder_targets:\n",
      "[6 5 4 6 9 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "x = next_feed()\n",
    "print('encoder_inputs:')\n",
    "print(x[encoder_inputs][0,:])\n",
    "print('encoder_inputs_length:')\n",
    "print(x[encoder_inputs_length][0])\n",
    "print('decoder_inputs:')\n",
    "print(x[decoder_inputs][0,:])\n",
    "print('decoder_targets:')\n",
    "print(x[decoder_targets][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.292070150375366\n",
      "  sample 1:\n",
      "    input     > [7 5 4 6 6 4 0 0]\n",
      "    predicted > [4 5 5 5 5 6 6 0 0]\n",
      "  sample 2:\n",
      "    input     > [7 6 9 4 7 7 9 0]\n",
      "    predicted > [4 5 7 6 6 6 5 2 2]\n",
      "  sample 3:\n",
      "    input     > [9 7 8 2 6 0 0 0]\n",
      "    predicted > [0 0 2 7 7 2 7 7 0]\n",
      "\n",
      "batch 100\n",
      "  minibatch loss: 1.3632570505142212\n",
      "  sample 1:\n",
      "    input     > [2 9 7 2 4 5 4 6]\n",
      "    predicted > [2 2 2 2 7 5 5 1 1]\n",
      "  sample 2:\n",
      "    input     > [8 5 2 6 4 4 0 0]\n",
      "    predicted > [7 7 7 7 7 1 1 0 0]\n",
      "  sample 3:\n",
      "    input     > [4 9 4 6 5 3 4 0]\n",
      "    predicted > [4 4 4 4 4 4 1 1 0]\n",
      "\n",
      "batch 200\n",
      "  minibatch loss: 0.8508769869804382\n",
      "  sample 1:\n",
      "    input     > [9 4 7 0 0 0 0 0]\n",
      "    predicted > [4 4 7 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [9 3 3 7 0 0 0 0]\n",
      "    predicted > [3 3 3 7 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [2 8 5 2 5 3 8 5]\n",
      "    predicted > [2 5 5 5 5 8 8 5 1]\n",
      "\n",
      "batch 300\n",
      "  minibatch loss: 0.6634529232978821\n",
      "  sample 1:\n",
      "    input     > [6 9 4 4 4 4 8 5]\n",
      "    predicted > [4 4 4 4 4 8 8 1 1]\n",
      "  sample 2:\n",
      "    input     > [6 7 8 0 0 0 0 0]\n",
      "    predicted > [6 7 8 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 3 9 2 8 3 9 0]\n",
      "    predicted > [9 9 9 2 9 9 9 1 0]\n",
      "\n",
      "batch 400\n",
      "  minibatch loss: 0.5118913054466248\n",
      "  sample 1:\n",
      "    input     > [3 2 4 0 0 0 0 0]\n",
      "    predicted > [3 2 4 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [9 8 8 4 4 9 6 0]\n",
      "    predicted > [9 8 4 4 4 6 6 1 0]\n",
      "  sample 3:\n",
      "    input     > [3 6 8 3 4 6 0 0]\n",
      "    predicted > [3 6 8 3 4 6 1 0 0]\n",
      "\n",
      "batch 500\n",
      "  minibatch loss: 0.45923367142677307\n",
      "  sample 1:\n",
      "    input     > [9 5 8 8 0 0 0 0]\n",
      "    predicted > [8 8 8 8 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 9 5 4 9 5 0 0]\n",
      "    predicted > [4 9 5 4 9 5 1 0 0]\n",
      "  sample 3:\n",
      "    input     > [8 2 9 7 0 0 0 0]\n",
      "    predicted > [8 2 9 7 1 0 0 0 0]\n",
      "\n",
      "batch 600\n",
      "  minibatch loss: 0.3611622154712677\n",
      "  sample 1:\n",
      "    input     > [4 5 8 0 0 0 0 0]\n",
      "    predicted > [4 5 8 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 5 8 3 0 0 0 0]\n",
      "    predicted > [5 5 8 3 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [7 8 2 0 0 0 0 0]\n",
      "    predicted > [7 8 2 1 0 0 0 0 0]\n",
      "\n",
      "batch 700\n",
      "  minibatch loss: 0.30410128831863403\n",
      "  sample 1:\n",
      "    input     > [6 6 8 2 6 7 8 0]\n",
      "    predicted > [6 6 8 6 6 7 8 1 0]\n",
      "  sample 2:\n",
      "    input     > [4 4 6 4 8 3 2 0]\n",
      "    predicted > [4 4 6 4 8 3 2 1 0]\n",
      "  sample 3:\n",
      "    input     > [7 6 8 3 0 0 0 0]\n",
      "    predicted > [7 6 8 3 1 0 0 0 0]\n",
      "\n",
      "batch 800\n",
      "  minibatch loss: 0.2709043323993683\n",
      "  sample 1:\n",
      "    input     > [5 7 7 9 8 0 0 0]\n",
      "    predicted > [7 7 7 9 8 1 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [7 5 4 5 9 6 0 0]\n",
      "    predicted > [5 5 4 5 9 6 1 0 0]\n",
      "  sample 3:\n",
      "    input     > [7 8 6 8 0 0 0 0]\n",
      "    predicted > [7 8 6 8 1 0 0 0 0]\n",
      "\n",
      "batch 900\n",
      "  minibatch loss: 0.2507738769054413\n",
      "  sample 1:\n",
      "    input     > [9 9 2 6 0 0 0 0]\n",
      "    predicted > [9 9 2 6 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [7 3 7 0 0 0 0 0]\n",
      "    predicted > [7 3 7 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 2 6 9 0 0 0 0]\n",
      "    predicted > [5 2 6 9 1 0 0 0 0]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 0.21785177290439606\n",
      "  sample 1:\n",
      "    input     > [7 3 9 3 0 0 0 0]\n",
      "    predicted > [7 3 9 3 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 6 4 0 0 0 0 0]\n",
      "    predicted > [5 6 4 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [3 6 9 9 3 7 9 0]\n",
      "    predicted > [3 9 9 9 3 7 9 1 0]\n",
      "\n",
      "batch 1100\n",
      "  minibatch loss: 0.2529984414577484\n",
      "  sample 1:\n",
      "    input     > [6 6 3 2 6 6 0 0]\n",
      "    predicted > [6 6 3 6 6 6 1 0 0]\n",
      "  sample 2:\n",
      "    input     > [9 7 3 8 2 2 0 0]\n",
      "    predicted > [9 7 2 2 2 2 1 0 0]\n",
      "  sample 3:\n",
      "    input     > [8 2 5 9 7 6 8 0]\n",
      "    predicted > [8 2 5 9 7 8 8 1 0]\n",
      "\n",
      "batch 1200\n",
      "  minibatch loss: 0.18093420565128326\n",
      "  sample 1:\n",
      "    input     > [8 8 3 2 7 5 3 7]\n",
      "    predicted > [8 8 3 2 7 5 3 7 1]\n",
      "  sample 2:\n",
      "    input     > [2 5 7 3 5 6 5 2]\n",
      "    predicted > [2 5 7 3 5 6 5 2 1]\n",
      "  sample 3:\n",
      "    input     > [8 5 5 8 2 6 9 0]\n",
      "    predicted > [8 5 5 8 2 6 9 1 0]\n",
      "\n",
      "batch 1300\n",
      "  minibatch loss: 0.20134498178958893\n",
      "  sample 1:\n",
      "    input     > [2 9 5 6 3 6 3 0]\n",
      "    predicted > [2 9 5 6 3 5 5 1 0]\n",
      "  sample 2:\n",
      "    input     > [8 4 7 3 0 0 0 0]\n",
      "    predicted > [8 4 7 3 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 8 9 7 5 3 4 0]\n",
      "    predicted > [6 8 9 7 5 3 4 1 0]\n",
      "\n",
      "batch 1400\n",
      "  minibatch loss: 0.15925544500350952\n",
      "  sample 1:\n",
      "    input     > [3 5 3 6 0 0 0 0]\n",
      "    predicted > [3 5 3 6 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [8 8 7 0 0 0 0 0]\n",
      "    predicted > [8 8 7 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [2 5 2 0 0 0 0 0]\n",
      "    predicted > [2 5 2 1 0 0 0 0 0]\n",
      "\n",
      "batch 1500\n",
      "  minibatch loss: 0.16710805892944336\n",
      "  sample 1:\n",
      "    input     > [9 8 4 2 8 4 0 0]\n",
      "    predicted > [9 8 4 2 8 4 1 0 0]\n",
      "  sample 2:\n",
      "    input     > [3 4 2 0 0 0 0 0]\n",
      "    predicted > [3 4 2 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [3 8 7 4 6 5 8 0]\n",
      "    predicted > [3 8 7 4 6 5 8 1 0]\n",
      "\n",
      "batch 1600\n",
      "  minibatch loss: 0.1651599407196045\n",
      "  sample 1:\n",
      "    input     > [8 9 2 0 0 0 0 0]\n",
      "    predicted > [8 9 2 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [3 4 4 3 3 0 0 0]\n",
      "    predicted > [3 4 3 3 3 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [8 2 5 7 0 0 0 0]\n",
      "    predicted > [8 2 5 7 1 0 0 0 0]\n",
      "\n",
      "batch 1700\n",
      "  minibatch loss: 0.13821403682231903\n",
      "  sample 1:\n",
      "    input     > [8 5 6 2 2 5 9 0]\n",
      "    predicted > [8 5 2 2 2 5 9 1 0]\n",
      "  sample 2:\n",
      "    input     > [3 3 8 9 0 0 0 0]\n",
      "    predicted > [3 3 8 9 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [4 3 2 0 0 0 0 0]\n",
      "    predicted > [4 3 2 1 0 0 0 0 0]\n",
      "\n",
      "batch 1800\n",
      "  minibatch loss: 0.1574953943490982\n",
      "  sample 1:\n",
      "    input     > [7 6 6 5 8 9 6 0]\n",
      "    predicted > [6 6 6 5 8 9 6 1 0]\n",
      "  sample 2:\n",
      "    input     > [9 7 9 5 6 6 5 8]\n",
      "    predicted > [9 7 9 5 6 6 5 8 1]\n",
      "  sample 3:\n",
      "    input     > [6 5 2 4 2 0 0 0]\n",
      "    predicted > [6 5 2 4 2 1 0 0 0]\n",
      "\n",
      "batch 1900\n",
      "  minibatch loss: 0.10590343177318573\n",
      "  sample 1:\n",
      "    input     > [3 7 3 5 7 0 0 0]\n",
      "    predicted > [3 7 3 5 7 1 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 3 7 9 0 0 0 0]\n",
      "    predicted > [4 3 7 9 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [7 6 4 9 3 5 0 0]\n",
      "    predicted > [7 6 4 9 3 5 1 0 0]\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 0.12807083129882812\n",
      "  sample 1:\n",
      "    input     > [9 2 5 5 3 2 6 0]\n",
      "    predicted > [2 2 5 5 3 2 6 1 0]\n",
      "  sample 2:\n",
      "    input     > [3 9 9 3 0 0 0 0]\n",
      "    predicted > [3 9 9 3 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 4 6 8 9 0 0 0]\n",
      "    predicted > [9 4 6 8 9 1 0 0 0]\n",
      "\n",
      "batch 2100\n",
      "  minibatch loss: 0.10965941846370697\n",
      "  sample 1:\n",
      "    input     > [6 7 8 6 7 0 0 0]\n",
      "    predicted > [6 7 8 6 7 1 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [8 5 8 3 7 2 0 0]\n",
      "    predicted > [8 5 8 3 7 2 1 0 0]\n",
      "  sample 3:\n",
      "    input     > [8 9 7 3 3 6 0 0]\n",
      "    predicted > [8 9 3 3 3 6 1 0 0]\n",
      "\n",
      "batch 2200\n",
      "  minibatch loss: 0.12249843776226044\n",
      "  sample 1:\n",
      "    input     > [2 5 5 8 2 0 0 0]\n",
      "    predicted > [2 5 5 8 2 1 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [6 6 9 7 8 5 2 2]\n",
      "    predicted > [6 6 9 7 8 5 2 2 1]\n",
      "  sample 3:\n",
      "    input     > [6 4 2 2 0 0 0 0]\n",
      "    predicted > [6 4 2 2 1 0 0 0 0]\n",
      "\n",
      "batch 2300\n",
      "  minibatch loss: 0.12412696331739426\n",
      "  sample 1:\n",
      "    input     > [3 6 8 2 0 0 0 0]\n",
      "    predicted > [3 6 8 2 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [8 2 5 7 4 3 2 2]\n",
      "    predicted > [8 2 5 7 2 2 2 2 1]\n",
      "  sample 3:\n",
      "    input     > [3 8 7 6 2 5 9 2]\n",
      "    predicted > [3 8 7 6 2 5 9 2 1]\n",
      "\n",
      "batch 2400\n",
      "  minibatch loss: 0.10621365159749985\n",
      "  sample 1:\n",
      "    input     > [9 9 2 8 6 2 5 0]\n",
      "    predicted > [9 9 2 8 6 2 5 1 0]\n",
      "  sample 2:\n",
      "    input     > [6 7 3 6 0 0 0 0]\n",
      "    predicted > [6 7 3 6 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [2 8 9 2 0 0 0 0]\n",
      "    predicted > [2 8 9 2 1 0 0 0 0]\n",
      "\n",
      "batch 2500\n",
      "  minibatch loss: 0.08667626231908798\n",
      "  sample 1:\n",
      "    input     > [2 4 5 5 9 0 0 0]\n",
      "    predicted > [2 4 5 5 9 1 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [9 7 2 9 0 0 0 0]\n",
      "    predicted > [9 7 2 9 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 6 5 3 2 0 0 0]\n",
      "    predicted > [9 6 5 3 2 1 0 0 0]\n",
      "\n",
      "batch 2600\n",
      "  minibatch loss: 0.09206113964319229\n",
      "  sample 1:\n",
      "    input     > [5 8 6 0 0 0 0 0]\n",
      "    predicted > [5 8 6 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [8 3 9 9 7 8 9 8]\n",
      "    predicted > [8 3 9 9 7 8 9 8 1]\n",
      "  sample 3:\n",
      "    input     > [3 7 7 4 9 2 5 0]\n",
      "    predicted > [3 7 7 4 9 2 5 1 0]\n",
      "\n",
      "batch 2700\n",
      "  minibatch loss: 0.10350560396909714\n",
      "  sample 1:\n",
      "    input     > [4 8 4 7 6 0 0 0]\n",
      "    predicted > [4 8 4 7 6 1 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 2 7 9 4 4 5 0]\n",
      "    predicted > [4 2 7 4 4 4 5 1 0]\n",
      "  sample 3:\n",
      "    input     > [8 4 7 3 8 7 2 0]\n",
      "    predicted > [8 4 7 3 8 7 2 1 0]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2800\n",
      "  minibatch loss: 0.06883814185857773\n",
      "  sample 1:\n",
      "    input     > [6 5 6 5 8 3 0 0]\n",
      "    predicted > [6 5 6 5 8 3 1 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 4 8 8 7 0 0 0]\n",
      "    predicted > [4 4 8 8 7 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 9 4 8 0 0 0 0]\n",
      "    predicted > [9 9 4 8 1 0 0 0 0]\n",
      "\n",
      "batch 2900\n",
      "  minibatch loss: 0.07545507699251175\n",
      "  sample 1:\n",
      "    input     > [4 4 7 0 0 0 0 0]\n",
      "    predicted > [4 4 7 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [3 2 9 2 8 9 6 0]\n",
      "    predicted > [3 2 9 2 8 9 6 1 0]\n",
      "  sample 3:\n",
      "    input     > [7 8 3 3 9 3 0 0]\n",
      "    predicted > [7 8 3 3 9 3 1 0 0]\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 0.08203375339508057\n",
      "  sample 1:\n",
      "    input     > [3 6 3 9 6 8 8 0]\n",
      "    predicted > [3 6 3 9 6 8 8 1 0]\n",
      "  sample 2:\n",
      "    input     > [9 3 6 9 7 0 0 0]\n",
      "    predicted > [9 3 6 9 7 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 7 9 5 7 3 0 0]\n",
      "    predicted > [9 7 9 5 7 3 1 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_batches = 3001\n",
    "batches_in_epoch = 100\n",
    "\n",
    "try:\n",
    "    # 一个epoch的learning\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "        \n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_output.predicted_ids, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs], predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.1133 after 300100 examples (batch_size=100)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ng58BAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11aec5c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.3003368,\n",
       " 2.2889502,\n",
       " 2.2850413,\n",
       " 2.2748864,\n",
       " 2.2703731,\n",
       " 2.2673423,\n",
       " 2.2566857,\n",
       " 2.2453299,\n",
       " 2.2410719,\n",
       " 2.2351534,\n",
       " 2.2234945,\n",
       " 2.2128115,\n",
       " 2.1971984,\n",
       " 2.1846426,\n",
       " 2.1983814,\n",
       " 2.1678469,\n",
       " 2.1685455,\n",
       " 2.1337967,\n",
       " 2.1348293,\n",
       " 2.1523581,\n",
       " 2.0821383,\n",
       " 2.0933614,\n",
       " 2.1138539,\n",
       " 2.0825896,\n",
       " 2.0139625,\n",
       " 2.0120842,\n",
       " 2.0004592,\n",
       " 2.0242736,\n",
       " 1.9941781,\n",
       " 1.9801381,\n",
       " 1.9836738,\n",
       " 1.9493093,\n",
       " 1.9226215,\n",
       " 1.8802541,\n",
       " 1.9159831,\n",
       " 1.8562158,\n",
       " 1.8766934,\n",
       " 1.8796484,\n",
       " 1.8341174,\n",
       " 1.852098,\n",
       " 1.8178946,\n",
       " 1.8324205,\n",
       " 1.7688894,\n",
       " 1.7121878,\n",
       " 1.8052138,\n",
       " 1.7862794,\n",
       " 1.7486801,\n",
       " 1.7697558,\n",
       " 1.7610954,\n",
       " 1.7614144,\n",
       " 1.7130306,\n",
       " 1.7516283,\n",
       " 1.6814686,\n",
       " 1.6751093,\n",
       " 1.728364,\n",
       " 1.6760588,\n",
       " 1.6265997,\n",
       " 1.667724,\n",
       " 1.6609944,\n",
       " 1.6701747,\n",
       " 1.6232169,\n",
       " 1.5436041,\n",
       " 1.6041431,\n",
       " 1.5650789,\n",
       " 1.6328917,\n",
       " 1.5884993,\n",
       " 1.5389982,\n",
       " 1.6198275,\n",
       " 1.5752089,\n",
       " 1.5325781,\n",
       " 1.6293771,\n",
       " 1.4337755,\n",
       " 1.6116514,\n",
       " 1.5187585,\n",
       " 1.5250646,\n",
       " 1.5261227,\n",
       " 1.529997,\n",
       " 1.5826259,\n",
       " 1.4981264,\n",
       " 1.479038,\n",
       " 1.4839875,\n",
       " 1.4482758,\n",
       " 1.488884,\n",
       " 1.4609228,\n",
       " 1.528109,\n",
       " 1.5688902,\n",
       " 1.4508051,\n",
       " 1.5322493,\n",
       " 1.513836,\n",
       " 1.3950603,\n",
       " 1.3924066,\n",
       " 1.4394227,\n",
       " 1.4453462,\n",
       " 1.4603899,\n",
       " 1.4203697,\n",
       " 1.403234,\n",
       " 1.3918753,\n",
       " 1.4291034,\n",
       " 1.4539236,\n",
       " 1.3453513,\n",
       " 1.3697059,\n",
       " 1.2579261,\n",
       " 1.3476819,\n",
       " 1.3606954,\n",
       " 1.3311806,\n",
       " 1.3476788,\n",
       " 1.3641267,\n",
       " 1.3218622,\n",
       " 1.3460327,\n",
       " 1.3433523,\n",
       " 1.332468,\n",
       " 1.2980341,\n",
       " 1.2477955,\n",
       " 1.2849045,\n",
       " 1.2700377,\n",
       " 1.3119746,\n",
       " 1.2700626,\n",
       " 1.323596,\n",
       " 1.2208583,\n",
       " 1.2219393,\n",
       " 1.2550169,\n",
       " 1.2559079,\n",
       " 1.2567283,\n",
       " 1.2198769,\n",
       " 1.2590275,\n",
       " 1.2637563,\n",
       " 1.2336153,\n",
       " 1.1870685,\n",
       " 1.2643284,\n",
       " 1.2129194,\n",
       " 1.2148997,\n",
       " 1.2580422,\n",
       " 1.2731878,\n",
       " 1.1867496,\n",
       " 1.1967901,\n",
       " 1.2390872,\n",
       " 1.2485938,\n",
       " 1.2401747,\n",
       " 1.1485064,\n",
       " 1.0954013,\n",
       " 1.14756,\n",
       " 1.1624012,\n",
       " 1.1654452,\n",
       " 1.1039681,\n",
       " 1.1288503,\n",
       " 1.1921862,\n",
       " 1.1192411,\n",
       " 1.0841974,\n",
       " 1.1724457,\n",
       " 1.0730705,\n",
       " 1.1257353,\n",
       " 1.097221,\n",
       " 1.1080036,\n",
       " 1.0665504,\n",
       " 1.0544302,\n",
       " 1.1426343,\n",
       " 1.0350151,\n",
       " 1.0684687,\n",
       " 1.0870972,\n",
       " 1.0436902,\n",
       " 1.094065,\n",
       " 1.0431452,\n",
       " 1.059237,\n",
       " 1.0938182,\n",
       " 1.0950704,\n",
       " 1.09304,\n",
       " 1.0144109,\n",
       " 1.1154233,\n",
       " 1.0088732,\n",
       " 1.0477624,\n",
       " 1.0697742,\n",
       " 1.020457,\n",
       " 1.0722283,\n",
       " 1.016427,\n",
       " 1.0134418,\n",
       " 0.96358287,\n",
       " 0.96816772,\n",
       " 1.0815251,\n",
       " 1.0276392,\n",
       " 0.99320513,\n",
       " 0.9776246,\n",
       " 0.9430272,\n",
       " 0.93347365,\n",
       " 1.0218247,\n",
       " 1.0932205,\n",
       " 1.0241438,\n",
       " 0.98439384,\n",
       " 0.94686979,\n",
       " 1.0247585,\n",
       " 0.96008444,\n",
       " 1.0163411,\n",
       " 0.94953448,\n",
       " 0.9406513,\n",
       " 0.99118429,\n",
       " 0.90373629,\n",
       " 0.90430039,\n",
       " 0.95272875,\n",
       " 0.85850489,\n",
       " 0.95810175,\n",
       " 0.87597293,\n",
       " 0.85511041,\n",
       " 0.86454564,\n",
       " 0.89934891,\n",
       " 0.87184083,\n",
       " 0.92704523,\n",
       " 0.85208589,\n",
       " 0.87825292,\n",
       " 0.84071958,\n",
       " 0.82956988,\n",
       " 0.90348685,\n",
       " 0.92384923,\n",
       " 0.87824589,\n",
       " 0.86149168,\n",
       " 0.88537067,\n",
       " 0.86502087,\n",
       " 0.88733077,\n",
       " 0.84832311,\n",
       " 0.89741832,\n",
       " 0.86165261,\n",
       " 0.87984496,\n",
       " 0.85604084,\n",
       " 0.85827392,\n",
       " 0.88631755,\n",
       " 0.84092486,\n",
       " 0.86969531,\n",
       " 0.89579713,\n",
       " 0.84186912,\n",
       " 0.86271471,\n",
       " 0.84845471,\n",
       " 0.87260449,\n",
       " 0.84142214,\n",
       " 0.86131322,\n",
       " 0.85548824,\n",
       " 0.83593631,\n",
       " 0.81870431,\n",
       " 0.77959186,\n",
       " 0.84365463,\n",
       " 0.7632916,\n",
       " 0.76836276,\n",
       " 0.76212281,\n",
       " 0.81818277,\n",
       " 0.74495429,\n",
       " 0.78210479,\n",
       " 0.78525925,\n",
       " 0.85698664,\n",
       " 0.82069677,\n",
       " 0.7230525,\n",
       " 0.81480998,\n",
       " 0.75807726,\n",
       " 0.73292607,\n",
       " 0.76237249,\n",
       " 0.72825956,\n",
       " 0.81549561,\n",
       " 0.77006781,\n",
       " 0.70172429,\n",
       " 0.74565464,\n",
       " 0.70681214,\n",
       " 0.79670793,\n",
       " 0.75868481,\n",
       " 0.75343835,\n",
       " 0.75253743,\n",
       " 0.74802828,\n",
       " 0.74286836,\n",
       " 0.78824288,\n",
       " 0.75830472,\n",
       " 0.70882744,\n",
       " 0.68859941,\n",
       " 0.73657483,\n",
       " 0.69825858,\n",
       " 0.79563856,\n",
       " 0.69810718,\n",
       " 0.76379478,\n",
       " 0.66648734,\n",
       " 0.72022337,\n",
       " 0.71169865,\n",
       " 0.75375789,\n",
       " 0.72894746,\n",
       " 0.72050101,\n",
       " 0.7389251,\n",
       " 0.64476031,\n",
       " 0.65283406,\n",
       " 0.73484987,\n",
       " 0.76211983,\n",
       " 0.70599061,\n",
       " 0.69104099,\n",
       " 0.67774326,\n",
       " 0.66684204,\n",
       " 0.71111709,\n",
       " 0.62252861,\n",
       " 0.67070156,\n",
       " 0.70716715,\n",
       " 0.64313585,\n",
       " 0.70574152,\n",
       " 0.69268572,\n",
       " 0.70850736,\n",
       " 0.70234758,\n",
       " 0.61956722,\n",
       " 0.66883099,\n",
       " 0.66462266,\n",
       " 0.6791541,\n",
       " 0.66833985,\n",
       " 0.60746962,\n",
       " 0.70423639,\n",
       " 0.64882863,\n",
       " 0.6971941,\n",
       " 0.68729478,\n",
       " 0.65222108,\n",
       " 0.6068421,\n",
       " 0.69355685,\n",
       " 0.63929564,\n",
       " 0.60648328,\n",
       " 0.61967081,\n",
       " 0.60263753,\n",
       " 0.64260781,\n",
       " 0.64588964,\n",
       " 0.61270738,\n",
       " 0.65531248,\n",
       " 0.59292859,\n",
       " 0.63186723,\n",
       " 0.64192706,\n",
       " 0.6095587,\n",
       " 0.66224289,\n",
       " 0.60323465,\n",
       " 0.62349582,\n",
       " 0.5506435,\n",
       " 0.66271698,\n",
       " 0.59063762,\n",
       " 0.60275877,\n",
       " 0.60858846,\n",
       " 0.64493918,\n",
       " 0.61396265,\n",
       " 0.61483383,\n",
       " 0.60388833,\n",
       " 0.61298233,\n",
       " 0.59215063,\n",
       " 0.58280098,\n",
       " 0.62618232,\n",
       " 0.58866078,\n",
       " 0.53504419,\n",
       " 0.59220934,\n",
       " 0.58539563,\n",
       " 0.59006292,\n",
       " 0.60921484,\n",
       " 0.57904989,\n",
       " 0.59731781,\n",
       " 0.59117216,\n",
       " 0.56447273,\n",
       " 0.57407689,\n",
       " 0.58875161,\n",
       " 0.62919974,\n",
       " 0.63343358,\n",
       " 0.62490469,\n",
       " 0.57015491,\n",
       " 0.58819538,\n",
       " 0.58757848,\n",
       " 0.57392132,\n",
       " 0.59284711,\n",
       " 0.62020481,\n",
       " 0.58517468,\n",
       " 0.59062231,\n",
       " 0.62578809,\n",
       " 0.55357218,\n",
       " 0.58792305,\n",
       " 0.57506573,\n",
       " 0.60036576,\n",
       " 0.59198505,\n",
       " 0.51520997,\n",
       " 0.55326056,\n",
       " 0.62051064,\n",
       " 0.57664549,\n",
       " 0.55698466,\n",
       " 0.54439968,\n",
       " 0.54405272,\n",
       " 0.5412429,\n",
       " 0.58177638,\n",
       " 0.51883608,\n",
       " 0.53951961,\n",
       " 0.53805149,\n",
       " 0.58191919,\n",
       " 0.55095845,\n",
       " 0.54218721,\n",
       " 0.52672392,\n",
       " 0.57015884,\n",
       " 0.55738986,\n",
       " 0.57449841,\n",
       " 0.54361737,\n",
       " 0.50754356,\n",
       " 0.52732289,\n",
       " 0.50139821,\n",
       " 0.53622967,\n",
       " 0.56473309,\n",
       " 0.58972001,\n",
       " 0.47640219,\n",
       " 0.55489826,\n",
       " 0.52613562,\n",
       " 0.47832799,\n",
       " 0.48358259,\n",
       " 0.51576376,\n",
       " 0.54154062,\n",
       " 0.52273744,\n",
       " 0.51328272,\n",
       " 0.49955621,\n",
       " 0.52506536,\n",
       " 0.54483891,\n",
       " 0.55137372,\n",
       " 0.57629144,\n",
       " 0.53667319,\n",
       " 0.51068586,\n",
       " 0.50861883,\n",
       " 0.5263114,\n",
       " 0.51700532,\n",
       " 0.51985925,\n",
       " 0.53288263,\n",
       " 0.48506612,\n",
       " 0.52749592,\n",
       " 0.57585025,\n",
       " 0.55797786,\n",
       " 0.42972738,\n",
       " 0.52454138,\n",
       " 0.53364444,\n",
       " 0.50976139,\n",
       " 0.5466373,\n",
       " 0.48574761,\n",
       " 0.50205857,\n",
       " 0.52974486,\n",
       " 0.47776386,\n",
       " 0.50708306,\n",
       " 0.50226301,\n",
       " 0.4652147,\n",
       " 0.46522662,\n",
       " 0.43984458,\n",
       " 0.48650932,\n",
       " 0.51602548,\n",
       " 0.53353137,\n",
       " 0.490334,\n",
       " 0.46024367,\n",
       " 0.54062164,\n",
       " 0.48615533,\n",
       " 0.50883192,\n",
       " 0.47506437,\n",
       " 0.51186103,\n",
       " 0.47516483,\n",
       " 0.45929953,\n",
       " 0.51152611,\n",
       " 0.4871614,\n",
       " 0.46192038,\n",
       " 0.48047596,\n",
       " 0.50171334,\n",
       " 0.48001608,\n",
       " 0.49054563,\n",
       " 0.48362353,\n",
       " 0.48322713,\n",
       " 0.5049693,\n",
       " 0.44265333,\n",
       " 0.40752488,\n",
       " 0.48618233,\n",
       " 0.49278864,\n",
       " 0.50015682,\n",
       " 0.44969639,\n",
       " 0.45117873,\n",
       " 0.45374125,\n",
       " 0.44004551,\n",
       " 0.51082557,\n",
       " 0.44517082,\n",
       " 0.47509462,\n",
       " 0.46835876,\n",
       " 0.48811138,\n",
       " 0.47193223,\n",
       " 0.48791206,\n",
       " 0.46440819,\n",
       " 0.50187892,\n",
       " 0.43948323,\n",
       " 0.44884828,\n",
       " 0.46328115,\n",
       " 0.47634473,\n",
       " 0.47216749,\n",
       " 0.44910038,\n",
       " 0.48154336,\n",
       " 0.48370335,\n",
       " 0.46457756,\n",
       " 0.41668272,\n",
       " 0.44938853,\n",
       " 0.41320929,\n",
       " 0.42265499,\n",
       " 0.47869274,\n",
       " 0.4067606,\n",
       " 0.44773528,\n",
       " 0.44945592,\n",
       " 0.45781437,\n",
       " 0.46493304,\n",
       " 0.46295741,\n",
       " 0.48575681,\n",
       " 0.42036727,\n",
       " 0.42931113,\n",
       " 0.46302035,\n",
       " 0.40175259,\n",
       " 0.44562474,\n",
       " 0.43100879,\n",
       " 0.45467865,\n",
       " 0.42393959,\n",
       " 0.46018067,\n",
       " 0.4107576,\n",
       " 0.44422716,\n",
       " 0.4096241,\n",
       " 0.41694215,\n",
       " 0.44172132,\n",
       " 0.44358048,\n",
       " 0.48176271,\n",
       " 0.44759271,\n",
       " 0.40431049,\n",
       " 0.41656151,\n",
       " 0.44578254,\n",
       " 0.44851318,\n",
       " 0.45365408,\n",
       " 0.43792149,\n",
       " 0.46050611,\n",
       " 0.38430321,\n",
       " 0.42819414,\n",
       " 0.44587979,\n",
       " 0.46343467,\n",
       " 0.4197253,\n",
       " 0.41227233,\n",
       " 0.42110601,\n",
       " 0.42360196,\n",
       " 0.42710254,\n",
       " 0.39802176,\n",
       " 0.42101747,\n",
       " 0.4523035,\n",
       " 0.43517154,\n",
       " 0.39043897,\n",
       " 0.42148244,\n",
       " 0.42948511,\n",
       " 0.38749647,\n",
       " 0.42674321,\n",
       " 0.45570335,\n",
       " 0.40832341,\n",
       " 0.4131811,\n",
       " 0.39619485,\n",
       " 0.39516899,\n",
       " 0.44607782,\n",
       " 0.37663263,\n",
       " 0.41870248,\n",
       " 0.3725006,\n",
       " 0.37052417,\n",
       " 0.39392352,\n",
       " 0.41280538,\n",
       " 0.39424124,\n",
       " 0.40829659,\n",
       " 0.42710516,\n",
       " 0.46095964,\n",
       " 0.40082595,\n",
       " 0.41992921,\n",
       " 0.38099504,\n",
       " 0.43510905,\n",
       " 0.40848908,\n",
       " 0.36522388,\n",
       " 0.38576782,\n",
       " 0.41840827,\n",
       " 0.42830554,\n",
       " 0.40354139,\n",
       " 0.39719957,\n",
       " 0.40036434,\n",
       " 0.36787289,\n",
       " 0.43132657,\n",
       " 0.39121985,\n",
       " 0.42113715,\n",
       " 0.40752229,\n",
       " 0.37405232,\n",
       " 0.41201395,\n",
       " 0.35962853,\n",
       " 0.43417263,\n",
       " 0.39868492,\n",
       " 0.38312012,\n",
       " 0.43100694,\n",
       " 0.38687947,\n",
       " 0.34559271,\n",
       " 0.37188396,\n",
       " 0.38782084,\n",
       " 0.45080918,\n",
       " 0.42313069,\n",
       " 0.39601365,\n",
       " 0.37575242,\n",
       " 0.39240792,\n",
       " 0.36150461,\n",
       " 0.41331023,\n",
       " 0.35466453,\n",
       " 0.4071545,\n",
       " 0.38319436,\n",
       " 0.37392977,\n",
       " 0.36889353,\n",
       " 0.37607971,\n",
       " 0.43713132,\n",
       " 0.35451064,\n",
       " 0.40981975,\n",
       " 0.34599328,\n",
       " 0.36559072,\n",
       " 0.36481112,\n",
       " 0.32458523,\n",
       " 0.37644598,\n",
       " 0.38862532,\n",
       " 0.36260369,\n",
       " 0.34431547,\n",
       " 0.37750381,\n",
       " 0.36877984,\n",
       " 0.3707152,\n",
       " 0.33956283,\n",
       " 0.38857713,\n",
       " 0.3966721,\n",
       " 0.3061977,\n",
       " 0.36008647,\n",
       " 0.3576732,\n",
       " 0.41185886,\n",
       " 0.36432582,\n",
       " 0.37714517,\n",
       " 0.32853454,\n",
       " 0.38464475,\n",
       " 0.35052362,\n",
       " 0.36433107,\n",
       " 0.35647574,\n",
       " 0.36976773,\n",
       " 0.31082174,\n",
       " 0.34819692,\n",
       " 0.37834927,\n",
       " 0.35707587,\n",
       " 0.41598636,\n",
       " 0.36230007,\n",
       " 0.37102327,\n",
       " 0.3275744,\n",
       " 0.36682564,\n",
       " 0.38437119,\n",
       " 0.35183722,\n",
       " 0.30782342,\n",
       " 0.34418479,\n",
       " 0.36762148,\n",
       " 0.3312358,\n",
       " 0.35878828,\n",
       " 0.35238111,\n",
       " 0.40392855,\n",
       " 0.33567637,\n",
       " 0.33664632,\n",
       " 0.33497494,\n",
       " 0.31875515,\n",
       " 0.3448942,\n",
       " 0.37934378,\n",
       " 0.32806152,\n",
       " 0.37614849,\n",
       " 0.33604175,\n",
       " 0.3482967,\n",
       " 0.33336908,\n",
       " 0.35732794,\n",
       " 0.31951299,\n",
       " 0.39304078,\n",
       " 0.36335695,\n",
       " 0.38103834,\n",
       " 0.38446739,\n",
       " 0.3456355,\n",
       " 0.33817959,\n",
       " 0.31469119,\n",
       " 0.35922608,\n",
       " 0.3620908,\n",
       " 0.32990262,\n",
       " 0.30912071,\n",
       " 0.33598813,\n",
       " 0.34463507,\n",
       " 0.32704359,\n",
       " 0.33991566,\n",
       " 0.35991564,\n",
       " 0.34358945,\n",
       " 0.34312499,\n",
       " 0.32912984,\n",
       " 0.31043667,\n",
       " 0.32299113,\n",
       " 0.35240725,\n",
       " 0.31474683,\n",
       " 0.30560589,\n",
       " 0.34060329,\n",
       " 0.36408085,\n",
       " 0.31240299,\n",
       " 0.34793869,\n",
       " 0.36130533,\n",
       " 0.3189837,\n",
       " 0.36051336,\n",
       " 0.30262953,\n",
       " 0.32705414,\n",
       " 0.29047415,\n",
       " 0.30604511,\n",
       " 0.34361884,\n",
       " 0.32032043,\n",
       " 0.31505847,\n",
       " 0.34291479,\n",
       " 0.31119516,\n",
       " 0.37434721,\n",
       " 0.29398268,\n",
       " 0.36353853,\n",
       " 0.31385022,\n",
       " 0.33979017,\n",
       " 0.30663994,\n",
       " 0.32031012,\n",
       " 0.30278823,\n",
       " 0.32771301,\n",
       " 0.30144453,\n",
       " 0.33988294,\n",
       " 0.29370037,\n",
       " 0.32786325,\n",
       " 0.34431401,\n",
       " 0.33223042,\n",
       " 0.35087773,\n",
       " 0.33289936,\n",
       " 0.2814545,\n",
       " 0.29586223,\n",
       " 0.34918967,\n",
       " 0.34192517,\n",
       " 0.35623848,\n",
       " 0.3178792,\n",
       " 0.32184944,\n",
       " 0.34055918,\n",
       " 0.32777029,\n",
       " 0.31519321,\n",
       " 0.28231364,\n",
       " 0.31090277,\n",
       " 0.30967733,\n",
       " 0.30127078,\n",
       " 0.30222833,\n",
       " 0.35109296,\n",
       " 0.31398061,\n",
       " 0.3251684,\n",
       " 0.3216967,\n",
       " 0.33486289,\n",
       " 0.30377117,\n",
       " 0.3435618,\n",
       " 0.33434665,\n",
       " 0.32151124,\n",
       " 0.30591565,\n",
       " 0.26411337,\n",
       " 0.317406,\n",
       " 0.25645468,\n",
       " 0.30967346,\n",
       " 0.31337103,\n",
       " 0.30334419,\n",
       " 0.33180147,\n",
       " 0.32548103,\n",
       " 0.35110453,\n",
       " 0.31310394,\n",
       " 0.32779688,\n",
       " 0.32184187,\n",
       " 0.30621168,\n",
       " 0.32561415,\n",
       " 0.30851623,\n",
       " 0.2978017,\n",
       " 0.30268952,\n",
       " 0.29273626,\n",
       " 0.3344669,\n",
       " 0.33264288,\n",
       " 0.30470026,\n",
       " 0.34840643,\n",
       " 0.29276687,\n",
       " 0.34741768,\n",
       " 0.3436718,\n",
       " 0.33916903,\n",
       " 0.29356694,\n",
       " 0.32321066,\n",
       " 0.29792517,\n",
       " 0.31658795,\n",
       " 0.31180882,\n",
       " 0.31393737,\n",
       " 0.29208359,\n",
       " 0.32290575,\n",
       " 0.27497372,\n",
       " 0.29462227,\n",
       " 0.28673431,\n",
       " 0.34129137,\n",
       " 0.2921381,\n",
       " 0.28253901,\n",
       " 0.27486593,\n",
       " 0.27119055,\n",
       " 0.32388169,\n",
       " 0.30903605,\n",
       " 0.30991909,\n",
       " 0.30747247,\n",
       " 0.32106605,\n",
       " 0.30272555,\n",
       " 0.31426427,\n",
       " 0.32422432,\n",
       " 0.28880769,\n",
       " 0.31059673,\n",
       " 0.33508086,\n",
       " 0.32106182,\n",
       " 0.29135182,\n",
       " 0.31310743,\n",
       " 0.29235271,\n",
       " 0.27902085,\n",
       " 0.3346087,\n",
       " 0.26330379,\n",
       " 0.30037373,\n",
       " 0.32320657,\n",
       " 0.29288736,\n",
       " 0.25094911,\n",
       " 0.29176411,\n",
       " 0.29376474,\n",
       " 0.28364143,\n",
       " 0.27321512,\n",
       " 0.27218613,\n",
       " 0.29871032,\n",
       " 0.27692643,\n",
       " 0.29818729,\n",
       " 0.32090953,\n",
       " 0.3156265,\n",
       " 0.2699402,\n",
       " 0.26850167,\n",
       " 0.30968106,\n",
       " 0.27451205,\n",
       " 0.28676942,\n",
       " 0.27323622,\n",
       " 0.27904028,\n",
       " 0.32638249,\n",
       " 0.25986674,\n",
       " 0.28280264,\n",
       " 0.31155249,\n",
       " 0.30242649,\n",
       " 0.27433413,\n",
       " 0.29063332,\n",
       " 0.32189548,\n",
       " 0.30268058,\n",
       " 0.27044877,\n",
       " 0.29498202,\n",
       " 0.30600256,\n",
       " 0.27779719,\n",
       " 0.29664856,\n",
       " 0.27976465,\n",
       " 0.28909749,\n",
       " 0.22248007,\n",
       " 0.2797232,\n",
       " 0.27122116,\n",
       " 0.28258815,\n",
       " 0.27495354,\n",
       " 0.29892954,\n",
       " 0.27960512,\n",
       " 0.2931743,\n",
       " 0.25075012,\n",
       " 0.3016108,\n",
       " 0.25214636,\n",
       " 0.24330567,\n",
       " 0.2923955,\n",
       " 0.26519793,\n",
       " 0.25763607,\n",
       " 0.26539859,\n",
       " 0.2907362,\n",
       " 0.26253411,\n",
       " 0.23174758,\n",
       " 0.25879794,\n",
       " 0.29232889,\n",
       " 0.33522603,\n",
       " 0.24663416,\n",
       " 0.25634545,\n",
       " 0.29383993,\n",
       " 0.2423265,\n",
       " 0.28735128,\n",
       " 0.26979846,\n",
       " 0.26771605,\n",
       " 0.28074083,\n",
       " 0.25850469,\n",
       " 0.2477261,\n",
       " 0.3014698,\n",
       " 0.30180916,\n",
       " 0.26760519,\n",
       " 0.30830288,\n",
       " 0.25356475,\n",
       " 0.27131534,\n",
       " 0.2481032,\n",
       " 0.26518151,\n",
       " 0.30781734,\n",
       " 0.2828269,\n",
       " 0.26452401,\n",
       " 0.26083705,\n",
       " 0.26378906,\n",
       " 0.28188962,\n",
       " 0.2783708,\n",
       " 0.25089344,\n",
       " 0.30538729,\n",
       " 0.25956202,\n",
       " 0.27113622,\n",
       " 0.28748116,\n",
       " 0.26316309,\n",
       " 0.27335429,\n",
       " 0.29059523,\n",
       " 0.28632364,\n",
       " 0.2555064,\n",
       " 0.24007443,\n",
       " 0.25932184,\n",
       " 0.28983134,\n",
       " 0.28557372,\n",
       " 0.24988583,\n",
       " 0.25808072,\n",
       " 0.26531473,\n",
       " 0.26121339,\n",
       " 0.22527535,\n",
       " 0.28559855,\n",
       " 0.26193297,\n",
       " 0.24573506,\n",
       " 0.27714044,\n",
       " 0.25494087,\n",
       " 0.25027043,\n",
       " 0.27093467,\n",
       " 0.26185414,\n",
       " 0.25100932,\n",
       " 0.26598147,\n",
       " 0.25422803,\n",
       " 0.24543542,\n",
       " 0.24469943,\n",
       " 0.27330101,\n",
       " 0.25972888,\n",
       " 0.31940907,\n",
       " 0.23213215,\n",
       " 0.25747469,\n",
       " 0.22551122,\n",
       " 0.24638122,\n",
       " 0.22584012,\n",
       " 0.2841574,\n",
       " 0.26037806,\n",
       " 0.28065303,\n",
       " 0.23174042,\n",
       " 0.26501435,\n",
       " 0.24685852,\n",
       " 0.24758473,\n",
       " 0.22748798,\n",
       " 0.2637668,\n",
       " 0.30567655,\n",
       " 0.24289863,\n",
       " 0.2609511,\n",
       " 0.26570538,\n",
       " 0.29099029,\n",
       " 0.2514061,\n",
       " 0.25653976,\n",
       " 0.25673991,\n",
       " 0.26598096,\n",
       " 0.24970402,\n",
       " 0.24036485,\n",
       " 0.25679198,\n",
       " 0.25867483,\n",
       " 0.26065522,\n",
       " 0.25932539,\n",
       " 0.23978508,\n",
       " 0.23678966,\n",
       " 0.25716063,\n",
       " 0.23867391,\n",
       " 0.24213137,\n",
       " 0.25902227,\n",
       " 0.27541858,\n",
       " 0.27943644,\n",
       " 0.24732134,\n",
       " 0.26899096,\n",
       " 0.23641305,\n",
       " 0.26862335,\n",
       " 0.26249275,\n",
       " 0.23831499,\n",
       " 0.2413328,\n",
       " 0.24025764,\n",
       " 0.23506247,\n",
       " 0.22647424,\n",
       " 0.24359009,\n",
       " 0.25341251,\n",
       " 0.23512787,\n",
       " 0.24342665,\n",
       " 0.24301852,\n",
       " 0.23840433,\n",
       " 0.2491557,\n",
       " 0.26749855,\n",
       " 0.23606649,\n",
       " 0.28204194,\n",
       " 0.23155762,\n",
       " 0.21845593,\n",
       " 0.25581133,\n",
       " 0.25246271,\n",
       " 0.27990374,\n",
       " 0.26691702,\n",
       " 0.22788058,\n",
       " 0.27148142,\n",
       " 0.21921888,\n",
       " 0.24821977,\n",
       " 0.23929077,\n",
       " 0.24742547,\n",
       " 0.25848815,\n",
       " 0.21525854,\n",
       " 0.23742059,\n",
       " 0.24042095,\n",
       " 0.22332011,\n",
       " 0.2438603,\n",
       " 0.25356951,\n",
       " 0.24725841,\n",
       " 0.24558935,\n",
       " 0.22301771,\n",
       " 0.26669195,\n",
       " 0.24274231,\n",
       " 0.25437808,\n",
       " 0.25202715,\n",
       " 0.2463112,\n",
       " 0.24860778,\n",
       " 0.28785482,\n",
       " 0.26773205,\n",
       " 0.27864841,\n",
       " ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
