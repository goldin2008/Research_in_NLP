{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置合成数据的特征\n",
    "total_size = 1000000\n",
    "\n",
    "def gen_data(size=total_size):\n",
    "    \"\"\" 按照上图生成合成序列数据\n",
    "    \n",
    "    Arguments:\n",
    "        size: input 和 output 序列的总长度\n",
    "    \n",
    "    Returns:\n",
    "        X, Y: input 和 output 序列，rank-1的numpy array （即，vector)\n",
    "    \"\"\"\n",
    "    \n",
    "    X = np.array(np.random.choice(2, size=(size,)))\n",
    "    Y = []\n",
    "    for i in range(size):\n",
    "        threshold = 0.5\n",
    "        if X[i-3] == 1:\n",
    "            threshold += 0.5\n",
    "        if X[i-8] == 1:\n",
    "            threshold -= 0.25\n",
    "        if np.random.rand() > threshold:\n",
    "            Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "    return X, np.array(Y)\n",
    "\n",
    "\n",
    "def gen_batch(raw_data, batch_size, num_steps):\n",
    "    \"\"\"产生minibatch数据\n",
    "    \n",
    "    Arguments:\n",
    "        raw_data: 所有的数据， (input, output) tuple\n",
    "        batch_size: 一个minibatch包含的样本数量；每个样本是一个sequence\n",
    "        num_step: 每个sequence样本的长度\n",
    "        \n",
    "    Returns:\n",
    "        一个generator，在一个tuple里面包含一个minibatch的输入，输出序列\n",
    "    \"\"\"\n",
    "    \n",
    "    raw_x, raw_y = raw_data\n",
    "    data_length = len(raw_x)\n",
    "\n",
    "    # partition raw data into batches and stack them vertically in a data matrix\n",
    "    batch_partition_length = data_length // batch_size\n",
    "    data_x = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    data_y = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    for i in range(batch_size):\n",
    "        data_x[i] = raw_x[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "        data_y[i] = raw_y[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "    # further divide batch partitions into num_steps for truncated backprop\n",
    "    epoch_size = batch_partition_length // num_steps\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        x = data_x[:, i * num_steps:(i + 1) * num_steps]\n",
    "        y = data_y[:, i * num_steps:(i + 1) * num_steps]\n",
    "        yield (x, y)\n",
    "\n",
    "def gen_epochs(n, num_steps, batch_size):\n",
    "    for i in range(n):\n",
    "        yield gen_batch(gen_data(), batch_size, num_steps)\n",
    "\n",
    "\n",
    "def rnn_model(num_steps, batch_size, state_size):\n",
    "    \"\"\" Build the rnn model computational graph.\n",
    "    \"\"\"\n",
    "    # Placeholders\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "        x = tf.placeholder(tf.int32, [None, num_steps],\n",
    "                           name='input_placeholder')\n",
    "        y = tf.placeholder(tf.int32, [None, num_steps],\n",
    "                           name='labels_placeholder')\n",
    "\n",
    "\n",
    "        # RNN Inputs， 将前面定义的placeholder输入到RNN cells\n",
    "        num_classes = 2\n",
    "        x_one_hot = tf.one_hot(x, num_classes) # [batch_size, num_steps，num_classes = 2]\n",
    "        rnn_inputs = tf.unstack(x_one_hot, axis=1) # [ num_steps, [batch_size, num_classes]]\n",
    "\n",
    "\n",
    "        # 手动实现 rnn_cell\n",
    "\n",
    "        with tf.variable_scope('rnn_cell'):\n",
    "            W = tf.get_variable('W', [num_classes + state_size, state_size], \n",
    "                               initializer = tf.random_normal_initializer(mean = 0.0, stddev = 0.01))\n",
    "            b = tf.get_variable('b', [state_size],\n",
    "                                initializer = tf.constant_initializer(0.0))\n",
    "        def rnn_cell(rnn_input, state):\n",
    "            with tf.variable_scope('rnn_cell', reuse=True):\n",
    "                W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "                b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "            return tf.tanh(tf.matmul(tf.concat([rnn_input, state], 1), W) + b)\n",
    "\n",
    "\n",
    "        # 对每个time frame应用rnn\n",
    "        rnn_outputs = []\n",
    "        init_state = tf.zeros([batch_size, state_size])\n",
    "\n",
    "        state = init_state\n",
    "        for rnn_input in rnn_inputs:\n",
    "            state = rnn_cell(rnn_input, state)\n",
    "            rnn_outputs.append(state)\n",
    "\n",
    "        final_state = rnn_outputs[-1]\n",
    "\n",
    "\n",
    "        # 从每个 time frame 的 hidden state\n",
    "        # 映射到每个 time frame 的最终 output（prediction）；\n",
    "        # 和CBOW或者SKIP-GRAM的最上一层相同\n",
    "\n",
    "        with tf.variable_scope('softmax'):\n",
    "            W = tf.get_variable('W', [state_size, num_classes])\n",
    "            b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "        logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "        predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "\n",
    "        # 计算损失函数\n",
    "        y_as_list = tf.unstack(y, num=num_steps, axis=1)\n",
    "        losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logit) for\n",
    "                  logit, label in zip(logits, y_as_list)]\n",
    "        total_loss = tf.reduce_mean(losses)\n",
    "\n",
    "        # 定义优化器\n",
    "        learning_rate = 0.1\n",
    "        train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "    return graph, losses, total_loss, final_state, train_step, x, y, init_state\n",
    "    #return graph\n",
    "\n",
    "\n",
    "def train_rnn(gen_epochs, num_steps = 10, batch_size = 32, state_size = 16):\n",
    "    \n",
    "    num_epochs = 5\n",
    "    verbose = False\n",
    "\n",
    "    graph, losses, total_loss, final_state, train_step, x, y, init_state = rnn_model(\n",
    "            num_steps, batch_size, state_size)\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps, batch_size)):\n",
    "            training_loss = 0\n",
    "            #training_state = np.zeros((batch_size, state_size))\n",
    "            if verbose:\n",
    "                print(\"\\nEPOCH\", idx)\n",
    "            for step, (X, Y) in enumerate(epoch):\n",
    "                training_state = np.zeros((batch_size, state_size))\n",
    "                tr_losses, training_loss_, training_state, _ = sess.run(\n",
    "                    [losses,\n",
    "                     total_loss,\n",
    "                     final_state,\n",
    "                     train_step],\n",
    "                    feed_dict={x:X, y:Y, init_state:training_state})\n",
    "                training_loss += training_loss_\n",
    "                if step % 500 == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"At step %d, average loss of last 500 steps are %f\\n\"\n",
    "                              % (step, training_loss/500.0))\n",
    "                    training_losses.append(training_loss/500.0)\n",
    "                    training_loss = 0\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_losses_3_16 = train_rnn(gen_epochs, num_steps = 3, batch_size = 32, state_size = 16)\n",
    "training_losses_3_4 = train_rnn(gen_epochs, num_steps = 3, batch_size = 32, state_size = 4)\n",
    "\n",
    "training_losses_4_16 = train_rnn(gen_epochs, num_steps = 4, batch_size = 32, state_size = 16)\n",
    "training_losses_4_4 = train_rnn(gen_epochs, num_steps = 4, batch_size = 32, state_size = 4)\n",
    "\n",
    "training_losses_8_16 = train_rnn(gen_epochs, num_steps = 8, batch_size = 32, state_size = 16)\n",
    "training_losses_8_4 = train_rnn(gen_epochs, num_steps = 8, batch_size = 32, state_size = 4)\n",
    "\n",
    "training_losses_9_16 = train_rnn(gen_epochs, num_steps = 9, batch_size = 32, state_size = 16)\n",
    "training_losses_9_4 = train_rnn(gen_epochs, num_steps = 9, batch_size = 32, state_size = 4)\n",
    "\n",
    "training_losses_10_16 = train_rnn(gen_epochs, num_steps = 10, batch_size = 32, state_size = 16)\n",
    "training_losses_10_4 = train_rnn(gen_epochs, num_steps = 10, batch_size = 32, state_size = 4)\n",
    "\n",
    "training_losses_20_16 = train_rnn(gen_epochs, num_steps = 20, batch_size = 32, state_size = 16)\n",
    "training_losses_20_4 = train_rnn(gen_epochs, num_steps = 20, batch_size = 32, state_size = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_losses_9_16)\n",
    "plt.title('num_step = 10, state_size = 16')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_losses_9_4)\n",
    "plt.title('num_step = 10, state_size = 4')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_losses_8_16)\n",
    "plt.title('num_step = 5, state_size = 16')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_losses_8_4)\n",
    "plt.title('num_step = 5, state_size = 4')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_losses_3_16)\n",
    "plt.title('num_step = 3, state_size = 16')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_losses_3_4)\n",
    "plt.title('num_step = 3, state_size = 4')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
